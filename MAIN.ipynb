{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1c4b62",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c917e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Imports'''\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "from rapidfuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944d9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reading Data'''\n",
    "data_dir_comments = r\"C:\\Users\\gungo\\OneDrive\\Desktop\\stocks_comments.ndjson\"\n",
    "data_dir_sub = r\"C:\\Users\\gungo\\OneDrive\\Desktop\\stocks_submissions.ndjson\"\n",
    "df_com = pd.read_json(data_dir_comments, lines=True)\n",
    "df_sub = pd.read_json(data_dir_sub, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3db05ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reducing Data'''\n",
    "df_com_reduced = df_com[['created_utc','score','body']]\n",
    "df_sub_reduced = df_sub[['created_utc','score','selftext']]\n",
    "\n",
    "df_com_reduced = df_com_reduced.rename(columns={'body': 'text'})\n",
    "df_sub_reduced = df_sub_reduced.rename(columns={'selftext': 'text'})\n",
    "\n",
    "df_merged = pd.concat([df_com_reduced, df_sub_reduced], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b071c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre-Processing'''\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans, tokenizes, removes stopwords, and stems text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    #text = text.lower()       Not needed for NER. Actually makes it worse\n",
    "    text = re.sub(r'&amp;#x200B;', '', text)\n",
    "    text = re.sub('&amp;', '', text) # remove some special characters from the data &amp; corresponds to &\n",
    "    text = re.sub(r'\\s+', ' ', text)  # eliminate duplicate whitespaces using regex\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', text)  # remove text in square brackets\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'\\binc\\b', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    #text = ' '.join(stemmer.stem(word) for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Preprocesses the 'body' or 'selftext' column and removes '[removed]' entries.\"\"\"\n",
    "    \n",
    "    text_col = 'text'\n",
    "\n",
    "    # Remove NaN and '[removed]' rows\n",
    "    df = df[df[text_col].notna()]\n",
    "    df = df[~df[text_col].str.contains(r'\\[removed\\]', na=False)]\n",
    "    df = df[~df[text_col].str.contains(r'\\[deleted\\]', na=False)]\n",
    "\n",
    "    # Apply text preprocessing\n",
    "    df['processed_text'] = df[text_col].apply(preprocess_text)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "pre_processed_df = preprocess(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bf27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1654041658</td>\n",
       "      <td>-1</td>\n",
       "      <td>Musk is a clown. He knew 50% of his followers ...</td>\n",
       "      <td>musk clown. knew 50% followers bots. knew twit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654041696</td>\n",
       "      <td>100</td>\n",
       "      <td>What's the cumulative short loss? $50 billion ...</td>\n",
       "      <td>what's cumulative short loss? $50 billion coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1654041706</td>\n",
       "      <td>2</td>\n",
       "      <td>Quantum computing is physics, but physics isn'...</td>\n",
       "      <td>quantum computing physics, physics business. p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654041743</td>\n",
       "      <td>62</td>\n",
       "      <td>MANGA</td>\n",
       "      <td>manga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654041839</td>\n",
       "      <td>8</td>\n",
       "      <td>AMD?\\n\\nThey sell on the merits of their produ...</td>\n",
       "      <td>amd? sell merits products, open source softwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1654041840</td>\n",
       "      <td>12</td>\n",
       "      <td>Highly coincidental that this drastic drop in ...</td>\n",
       "      <td>highly coincidental drastic drop price happene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1654041851</td>\n",
       "      <td>2</td>\n",
       "      <td>Of course you can time the market, on a macro ...</td>\n",
       "      <td>course time market, macro basis - follow fed. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1654041856</td>\n",
       "      <td>1</td>\n",
       "      <td>However the issue is with the decay.  It may s...</td>\n",
       "      <td>however issue decay. may show 100% gains hits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1654041865</td>\n",
       "      <td>13</td>\n",
       "      <td>They exclude the 5% they know about.\\n\\nAnd it...</td>\n",
       "      <td>exclude 5% know about. matter. advertisers get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1654041907</td>\n",
       "      <td>11</td>\n",
       "      <td>The board dgaf what Dorsey days.</td>\n",
       "      <td>board dgaf dorsey days.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1654041958</td>\n",
       "      <td>1</td>\n",
       "      <td>Smh</td>\n",
       "      <td>smh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1654041964</td>\n",
       "      <td>1</td>\n",
       "      <td>This explains why the metaverse ETF META (that...</td>\n",
       "      <td>explains metaverse etf meta (that flat novembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1654041969</td>\n",
       "      <td>2</td>\n",
       "      <td>Yea agree buying a house outright is not the b...</td>\n",
       "      <td>yea agree buying house outright best use capital.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1654041986</td>\n",
       "      <td>7</td>\n",
       "      <td>You can't waive...jeebus...\\n\\nTwitter is in t...</td>\n",
       "      <td>can't waive...jeebus... twitter clear. disclai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1654042051</td>\n",
       "      <td>1</td>\n",
       "      <td>I've been buying oil since the mid 60s and tha...</td>\n",
       "      <td>buying oil since mid 60s reasoning: -shale ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1654042058</td>\n",
       "      <td>1</td>\n",
       "      <td>I work in the camping industry. And yes they'r...</td>\n",
       "      <td>work camping industry. yes gonna keep buying r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1654042079</td>\n",
       "      <td>2</td>\n",
       "      <td>Agree its a lousy time to buy a house</td>\n",
       "      <td>agree lousy time buy house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1654042092</td>\n",
       "      <td>1</td>\n",
       "      <td>Yeah I am so good at stock picking maybe I sho...</td>\n",
       "      <td>yeah good stock picking maybe gamble one time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1654042092</td>\n",
       "      <td>9</td>\n",
       "      <td>Oof</td>\n",
       "      <td>oof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1654042100</td>\n",
       "      <td>9</td>\n",
       "      <td>He seems to already have a liquidated billions...</td>\n",
       "      <td>seems already liquidated billions tesla shares...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc  score                                               text  \\\n",
       "0    1654041658     -1  Musk is a clown. He knew 50% of his followers ...   \n",
       "1    1654041696    100  What's the cumulative short loss? $50 billion ...   \n",
       "2    1654041706      2  Quantum computing is physics, but physics isn'...   \n",
       "3    1654041743     62                                              MANGA   \n",
       "4    1654041839      8  AMD?\\n\\nThey sell on the merits of their produ...   \n",
       "5    1654041840     12  Highly coincidental that this drastic drop in ...   \n",
       "6    1654041851      2  Of course you can time the market, on a macro ...   \n",
       "7    1654041856      1  However the issue is with the decay.  It may s...   \n",
       "8    1654041865     13  They exclude the 5% they know about.\\n\\nAnd it...   \n",
       "9    1654041907     11                   The board dgaf what Dorsey days.   \n",
       "10   1654041958      1                                                Smh   \n",
       "11   1654041964      1  This explains why the metaverse ETF META (that...   \n",
       "12   1654041969      2  Yea agree buying a house outright is not the b...   \n",
       "13   1654041986      7  You can't waive...jeebus...\\n\\nTwitter is in t...   \n",
       "14   1654042051      1  I've been buying oil since the mid 60s and tha...   \n",
       "15   1654042058      1  I work in the camping industry. And yes they'r...   \n",
       "16   1654042079      2              Agree its a lousy time to buy a house   \n",
       "17   1654042092      1  Yeah I am so good at stock picking maybe I sho...   \n",
       "18   1654042092      9                                                Oof   \n",
       "19   1654042100      9  He seems to already have a liquidated billions...   \n",
       "\n",
       "                                       processed_text  \n",
       "0   musk clown. knew 50% followers bots. knew twit...  \n",
       "1   what's cumulative short loss? $50 billion coun...  \n",
       "2   quantum computing physics, physics business. p...  \n",
       "3                                               manga  \n",
       "4   amd? sell merits products, open source softwar...  \n",
       "5   highly coincidental drastic drop price happene...  \n",
       "6   course time market, macro basis - follow fed. ...  \n",
       "7   however issue decay. may show 100% gains hits ...  \n",
       "8   exclude 5% know about. matter. advertisers get...  \n",
       "9                             board dgaf dorsey days.  \n",
       "10                                                smh  \n",
       "11  explains metaverse etf meta (that flat novembe...  \n",
       "12  yea agree buying house outright best use capital.  \n",
       "13  can't waive...jeebus... twitter clear. disclai...  \n",
       "14  buying oil since mid 60s reasoning: -shale ove...  \n",
       "15  work camping industry. yes gonna keep buying r...  \n",
       "16                         agree lousy time buy house  \n",
       "17  yeah good stock picking maybe gamble one time ...  \n",
       "18                                                oof  \n",
       "19  seems already liquidated billions tesla shares...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Display'''\n",
    "pre_processed_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d007d",
   "metadata": {},
   "source": [
    "NAMED ENTITY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34d5b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_path = r'Top_100.csv'\n",
    "Top_100 = pd.read_csv(top100_path)\n",
    "\n",
    "Top_100.columns = [col.strip().lower() for col in Top_100.columns]\n",
    "\n",
    "# Create dictionaries for fast lookups\n",
    "ticker_to_name = dict(zip(Top_100['symbol'].str.upper(), Top_100['name']))\n",
    "valid_tickers = set(ticker_to_name.keys())\n",
    "company_names = [name.lower() for name in ticker_to_name.values()]\n",
    "name_to_ticker = {name.lower(): symbol for symbol, name in ticker_to_name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42dbee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#to be ran per comment\n",
    "def extract_ner_entities(model, text, similarity_threshold=90):\n",
    "    \n",
    "    BLACKLIST = {'ev', 'covid', 'etf', 'nyse', 'sec', 'spac', 'fda', 'treasury', 'covid-19', 'rrsp', 'tfsa','fed'}\n",
    "    doc = model(text)\n",
    "    detected_companies = []\n",
    "\n",
    "    #Detect companies via spaCy NER\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" and ent.text.lower() not in BLACKLIST:\n",
    "            org_name = ent.text.strip()\n",
    "            # Fuzzy match against official company names from csv file\n",
    "            match, score, _ = process.extractOne(org_name.lower(), company_names, scorer=fuzz.token_sort_ratio)\n",
    "            if score >= similarity_threshold:\n",
    "                matched_ticker = name_to_ticker[match]\n",
    "                canonical_name = ticker_to_name[matched_ticker]\n",
    "                detected_companies.append(canonical_name)\n",
    "            else:\n",
    "                #keeps companies not in csv file maybe delete later\n",
    "                detected_companies.append(org_name)\n",
    "\n",
    "    # --- Match stock tickers in text ---\n",
    "    for token in doc:\n",
    "        token_text = token.text.strip()\n",
    "\n",
    "        # Handle tickers with $ prefix, e.g. $AAPL\n",
    "        if token_text.startswith(\"$\"):\n",
    "            token_text = token_text[1:]\n",
    "\n",
    "        # Check if itâ€™s a valid ticker symbol\n",
    "        if token_text in valid_tickers:\n",
    "            company_name = ticker_to_name.get(token_text)\n",
    "            detected_companies.append(company_name)\n",
    "\n",
    "\n",
    "    return list(set(detected_companies))\n",
    "\n",
    "\n",
    "def get_dict_top_companies(dataset, column_name, top_companies=10):\n",
    "    company_counter = dict()\n",
    "    for companies in dataset[column_name]:\n",
    "        for company in companies:\n",
    "            if company in company_counter:  \n",
    "                company_counter[company] += 1\n",
    "            else:\n",
    "                company_counter[company] = 1\n",
    "    sorted_dict = dict(sorted(company_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "    top = dict()\n",
    "    for company, count in list(sorted_dict.items())[:top_companies]:\n",
    "        top[company] = count\n",
    "\n",
    "    return top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5e7d78",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_ner_entities(nlp, text, similarity_threshold=\u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m pre_processed_df[\u001b[33m\"\u001b[39m\u001b[33mCompanies\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mpre_processed_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessed_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msafe_extract\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36msafe_extract\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mextract_ner_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mextract_ner_entities\u001b[39m\u001b[34m(model, text, similarity_threshold)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_ner_entities\u001b[39m(model, text, similarity_threshold=\u001b[32m90\u001b[39m):\n\u001b[32m     11\u001b[39m     BLACKLIST = {\u001b[33m'\u001b[39m\u001b[33mev\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcovid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33metf\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnyse\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msec\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mspac\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfda\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtreasury\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcovid-19\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrrsp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtfsa\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mfed\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     doc = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     detected_companies = []\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m#Detect companies via spaCy NER\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\pipeline\\tok2vec.py:121\u001b[39m, in \u001b[36mTok2Vec.predict\u001b[39m\u001b[34m(self, docs)\u001b[39m\n\u001b[32m    119\u001b[39m     width = \u001b[38;5;28mself\u001b[39m.model.get_dim(\u001b[33m\"\u001b[39m\u001b[33mnO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.model.ops.alloc((\u001b[32m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m tokvecs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, Xseq, is_train)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.layers[\u001b[32m0\u001b[39m](Xseq, is_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[39m, in \u001b[36m_list_forward\u001b[39m\u001b[34m(model, Xs, is_train)\u001b[39m\n\u001b[32m     75\u001b[39m lengths = NUMPY_OPS.asarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[32m     76\u001b[39m Xf = layer.ops.flatten(Xs, pad=pad)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m Yf, get_dXf = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n\u001b[32m     80\u001b[39m     dYf = layer.ops.flatten(dYs, pad=pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\layers\\residual.py:41\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output + dX\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m Y, backprop_layer = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] + Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "    \u001b[31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\layers\\maxout.py:52\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     50\u001b[39m W = model.get_param(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m W = model.ops.reshape2f(W, nO * nP, nI)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m Y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m Y += model.ops.reshape1f(b, nO * nP)\n\u001b[32m     54\u001b[39m Z = model.ops.reshape3f(Y, Y.shape[\u001b[32m0\u001b[39m], nO, nP)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Ensure every cell is a string (NaN -> \"\")\n",
    "pre_processed_df['processed_text'] = pre_processed_df['processed_text'].fillna(\"\").astype(str)\n",
    "\n",
    "# Safe wrapper so extract_ner_entities always receives a string\n",
    "def safe_extract(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    return extract_ner_entities(nlp, text, similarity_threshold=60)\n",
    "\n",
    "pre_processed_df[\"Companies\"] = pre_processed_df['processed_text'].apply(safe_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d908e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_dict_top_companies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Submissions ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m top_dict = \u001b[43mget_dict_top_companies\u001b[49m(pre_processed_df, \u001b[33m\"\u001b[39m\u001b[33mCompanies\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m top_set = \u001b[38;5;28mset\u001b[39m(top_dict.keys())\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTop companies: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_dict_top_companies' is not defined"
     ]
    }
   ],
   "source": [
    "top_dict = get_dict_top_companies(pre_processed_df, \"Companies\")\n",
    "top_set = set(top_dict.keys())\n",
    "print(f\"Top companies: {top_set}\")\n",
    "\n",
    "masked = pre_processed_df['Companies'].apply(lambda lst: bool(top_set.intersection(lst)))\n",
    "filtered_df = pre_processed_df[masked].copy()\n",
    "\n",
    "exploded_sub = filtered_df.explode('Companies')\n",
    "\n",
    "# Keep only rows for top companies \n",
    "exploded_sub = exploded_sub[exploded_sub['Companies'].isin(top_set)].copy()\n",
    "\n",
    "# Create separate dataframes for each top company \n",
    "dfs_by_company = {}\n",
    "os.makedirs(\"companies_csv\", exist_ok=True)\n",
    "\n",
    "for company in top_set:\n",
    "    dfs_by_company[company] = exploded_sub[exploded_sub['Companies'] == company].copy()\n",
    "    file_path = os.path.join(\"companies_csv\", f\"{company}_company_data.csv\")\n",
    "    dfs_by_company[company].to_csv(file_path, index=False)\n",
    "    print(f\"{company}: {len(dfs_by_company[company])} rows saved to {file_path}\")\n",
    "\n",
    "print(\"\\nPreview of each company's dataframe:\\n\")\n",
    "for company, df_company in dfs_by_company.items():\n",
    "    print(f\"=== {company.upper()} ({len(df_company)} rows) ===\")\n",
    "    display(df_company.head(3))   \n",
    "    print(\"\\n\")\n",
    "\n",
    "filtered_df.to_csv(\"filtered_top_companies.csv\", index=False)  #not really needed, just the dataframe with all top companies submissions\n",
    "print(\"Saved filtered_top_companies.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ea6ee",
   "metadata": {},
   "source": [
    "## FinBERT Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aef2d3",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e920ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import random\n",
    "\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"    # avoid TF backend import\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"  # avoid Flax\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe1df9",
   "metadata": {},
   "source": [
    "\n",
    "#### FinBERT Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6892e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already loaded; skipping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL = \"ProsusAI/finbert\"\n",
    "\n",
    "if \"model\" not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded FinBERT on {DEVICE}.\")\n",
    "else:\n",
    "    # (optional) if CUDA becomes available later, move it\n",
    "    new_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if str(next(model.parameters()).device) != new_device:\n",
    "        model.to(new_device).eval()\n",
    "        DEVICE = new_device\n",
    "        print(f\"Moved model to {DEVICE}.\")\n",
    "    else:\n",
    "        print(\"Model already loaded; skipping.\")\n",
    "\n",
    "id2label = model.config.id2label  # {0:'positive',1:'negative',2:'neutral'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fc5cf",
   "metadata": {},
   "source": [
    "\n",
    "#### Function Definitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe549c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def finbert_proba(texts, batch_size=32, device=DEVICE, max_length=256):\n",
    "    \"\"\"\n",
    "    Return a list of dicts: [{'negative': p, 'neutral': p, 'positive': p}, ...]\n",
    "    \"\"\"\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            ).to(device)\n",
    "            logits = model(**enc).logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().tolist()\n",
    "            for p in probs:\n",
    "                # map indices -> labels dict\n",
    "                out = {id2label[j]: float(p[j]) for j in range(len(p))}\n",
    "                # ensure all three keys exist\n",
    "                for k in (\"negative\", \"neutral\", \"positive\"):\n",
    "                    out.setdefault(k, 0.0)\n",
    "                all_probs.append(out)\n",
    "    return all_probs\n",
    "\n",
    "def sentiment_label(texts, neutrality_threshold=1):\n",
    "    \"\"\"\n",
    "    Map each text to 'positive'/'negative'/'neutral' after thresholding.\n",
    "    \"\"\"\n",
    "    probs = finbert_proba(texts)\n",
    "    labels = []\n",
    "    for p in probs:\n",
    "        if p[\"neutral\"] >= neutrality_threshold:\n",
    "            labels.append(\"neutral\")\n",
    "        else:\n",
    "            labels.append(\"positive\" if p[\"positive\"] > p[\"negative\"] else \"negative\")\n",
    "    return labels\n",
    "\n",
    "def top2_margin(p: dict) -> float:\n",
    "    \"\"\"\n",
    "    Return max_prob - second_prob in [0,1]. p has keys 'negative','neutral','positive'.\n",
    "    \"\"\"\n",
    "    vals = sorted([p[\"negative\"], p[\"neutral\"], p[\"positive\"]], reverse=True)\n",
    "    return float(vals[0] - vals[1])\n",
    "\n",
    "def sentiment_score(texts):\n",
    "    \"\"\"\n",
    "    Continuous score per text: 0 if too neutral (currently 100% neutral or pos = neg), else (pos - neg).\n",
    "    Useful for weighting by Reddit score later.\n",
    "    \"\"\"\n",
    "    probs = finbert_proba(texts)\n",
    "    scores = []\n",
    "    for p in probs:\n",
    "        base = (p[\"positive\"] - p[\"negative\"])           \n",
    "        neutral_damp = (1.0 - p[\"neutral\"])              \n",
    "        margin_damp = top2_margin(p)                           \n",
    "        scores.append(base * neutral_damp * margin_damp)\n",
    "    return scores\n",
    "\n",
    "# tanh with 30D window\n",
    "def tanh_scale_series(scores, timestamps=None, percentile=95, min_periods=14):\n",
    "    \"\"\"\n",
    "    30D rolling P95 tanh scaling.\n",
    "    - scores: array-like of signed Reddit scores (ups - downs)\n",
    "    - timestamps: array-like of datetimes aligned with scores (if None -> global P95)\n",
    "    Returns:\n",
    "      w: np.ndarray in [-1, 1] (tanh-scaled, sign preserved)\n",
    "      k_used: np.ndarray of per-row k values (rolling P95 of |score|)\n",
    "    \"\"\"\n",
    "    s = np.asarray(scores, dtype=float)\n",
    "    if s.size == 0:\n",
    "        return s.astype(float), np.array([], dtype=float)\n",
    "    s = np.nan_to_num(s, nan=0.0)\n",
    "\n",
    "    if timestamps is not None:\n",
    "        ts = pd.to_datetime(pd.Series(timestamps), utc=True, errors=\"coerce\")\n",
    "        abs_s = pd.Series(np.abs(s), index=ts).sort_index()\n",
    "        roll_p = abs_s.rolling(\"30D\", min_periods=min_periods).quantile(percentile/100.0)\n",
    "        g_p = abs_s.quantile(percentile/100.0) if len(abs_s) else 1.0\n",
    "        k_series = roll_p.fillna(g_p).clip(lower=1.0)\n",
    "        # align per-row k back to original order (by timestamps)\n",
    "        k_used = k_series.reindex(ts).to_numpy()\n",
    "        # fallback for NaT rows\n",
    "        k_used = np.where(np.isfinite(k_used), k_used, max(g_p, 1.0))\n",
    "    else:\n",
    "        # global P95\n",
    "        g_p = np.percentile(np.abs(s), percentile) if s.size else 1.0\n",
    "        k_used = np.full_like(s, fill_value=max(g_p, 1.0), dtype=float)\n",
    "\n",
    "    w = np.tanh(s / k_used)\n",
    "    return w.astype(float), k_used.astype(float)\n",
    "\n",
    "# tanh scaling over whole dataset\n",
    "# def tanh_scale_series(scores, k=None, percentile=95):\n",
    "#     s = np.asarray(scores, dtype=float)\n",
    "#     s = np.nan_to_num(s, nan=0.0)\n",
    "#     if k is None:\n",
    "#         # choose a robust scale from your own data\n",
    "#         P = np.percentile(np.abs(s), percentile) if s.size else 1.0\n",
    "#         k = max(P, 1.0)\n",
    "#     w = np.tanh(s / k)  # w in [-1, 1], sign preserved\n",
    "#     return w, float(k)\n",
    "\n",
    "# without Tanh scaling.\n",
    "# def score_multiplier(raw_sentiment_scores, reddit_scores):\n",
    "#     \"\"\"\n",
    "#     Multiplies score per text by their Reddit score (Upvotes-Downvotes). \n",
    "#     \"\"\"\n",
    "#     scaled_reddit_scores = reddit_scores * w\n",
    "#     for i in range(len(raw_sentiment_scores)):\n",
    "#         scaled_sentiment_scores.append(raw_sentiment_scores[i] * scaled_reddit_scores[i])\n",
    "#     return scaled_sentiment_scores\n",
    "\n",
    "def score_multiplier(raw_sentiment_scores, reddit_scores, percentile=95):\n",
    "    \"\"\"\n",
    "    raw_sentiment_scores: list/array of sentiment scores in [-1, 1]\n",
    "    reddit_scores: list/array of signed Reddit scores (ups - downs)\n",
    "    returns: list of crowd-adjusted scores in [-1, 1]\n",
    "    \"\"\"\n",
    "    rs = np.asarray(raw_sentiment_scores, dtype=float)\n",
    "    w, k_used = tanh_scale_series(reddit_scores, percentile=percentile)\n",
    "    # elementwise combine; DO NOT multiply by raw reddit_scores again\n",
    "    return (rs * w).tolist(), k_used\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fab554",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Running the Model on Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c4d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# proper, multiple datasets lines below\n",
    "# file_locations ={\"Amazon\" : r\"C:\\Users\\ftomi\\Documents\\GitHub\\NLP-Group-10\\companies_csv\\Amazon_submissions.csv\", \n",
    "#                  \"Apple\" : r\"C:\\Users\\ftomi\\Documents\\GitHub\\NLP-Group-10\\companies_csv\\Apple_submissions.csv\", \n",
    "#                  \"Tesla\" : r\"C:\\Users\\ftomi\\Documents\\GitHub\\NLP-Group-10\\companies_csv\\Tesla_submissions.csv\"} \n",
    "\n",
    "# singular test line below\n",
    "file_locations = {\"Apple\" : r\"C:\\Users\\ftomi\\Documents\\GitHub\\NLP-Group-10\\companies_csv\\Apple_submissions.csv\"}\n",
    "\n",
    "for fl in file_locations:\n",
    "\n",
    "    df = pd.read_csv(file_locations[fl])\n",
    "\n",
    "    dates = []\n",
    "    texts = []\n",
    "    upvotes = [] \n",
    "    scaled_sentiment_scores = []\n",
    "    raw_sentiment_scores = []\n",
    "\n",
    "    data = df.to_dict('index')\n",
    "    for i in range(len(data)):\n",
    "        texts.append(data[i]['selftext'])\n",
    "        dates.append(datetime.fromtimestamp(data[i]['created_utc'], tz=timezone.utc))\n",
    "        upvotes.append(data[i]['score'])\n",
    "\n",
    "    probs = finbert_proba(texts)\n",
    "    raw_sentiment_scores = sentiment_score(texts)\n",
    "    scaled_sentiment_scores, k_used = score_multiplier(sentiment_score(texts), upvotes)\n",
    "    \n",
    "    # save as dataframe\n",
    "    sentiment_df = pd.DataFrame(dict(scaled_sentiment = scaled_sentiment_scores, created_utc = dates))\n",
    "    sentiment_df.to_csv(f\"{fl}_sentiment_data.csv\", index=False)\n",
    "\n",
    "    #show and save figure\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(dates, raw_sentiment_scores, label = \"raw\",  linestyle=\":\", linewidth=0.8)\n",
    "    plt.plot(dates, scaled_sentiment_scores, label = \"scaled by reddit score\")\n",
    "    plt.title(f\"Sentiment Towards {fl} on r/stocks\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Total Sentiment\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{fl}_plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    #show and save figure\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(dates, raw_sentiment_scores, label = \"raw\",  linestyle=\":\", linewidth=0.8)\n",
    "    plt.plot(dates, scaled_sentiment_scores, label = \"scaled by reddit score\")\n",
    "    plt.title(f\"Sentiment Towards {fl} on r/stocks\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Total Sentiment\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{fl}_plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19641f3f",
   "metadata": {},
   "source": [
    "\n",
    "## FinBERT Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594cc30",
   "metadata": {},
   "source": [
    "\n",
    "#### Sample Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a360f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Due to large file size, it is sampled first\n",
    "file, original # rows, app. % of rows kept, app. # rows kept:\n",
    "Submissions, 5765, 17.3461, 1000\n",
    "Comments, 616146, 0.1623, 1000\n",
    "'''\n",
    "p1 = 0.173461\n",
    "p2 = 0.001623\n",
    "\n",
    "submissions = pd.read_csv(\n",
    "    r\"C:\\Users\\ftomi\\Desktop\\NLP project\\submissions.csv\",\n",
    "    skiprows=lambda i: i > 0 and random.random() > p1,  # skip most rows\n",
    ")\n",
    "\n",
    "comments = pd.read_csv(\n",
    "    r\"C:\\Users\\ftomi\\Desktop\\NLP project\\comments.csv\",\n",
    "    skiprows=lambda i: i > 0 and random.random() > p2,  # skip most rows\n",
    ")\n",
    "\n",
    "dfs = [submissions, comments]\n",
    "print(len(submissions),len(comments))\n",
    "\n",
    "COL_IDX = 4 \n",
    "\n",
    "def drop_in_loop(df):\n",
    "    rows_to_drop = [] \n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row.iloc[COL_IDX]).strip()\n",
    "        is_empty = (text == '[]' or text == '')\n",
    "        if is_empty:\n",
    "            rows_to_drop.append(idx)\n",
    "    return df.drop(index=rows_to_drop).reset_index(drop=True)\n",
    "\n",
    "submissions = drop_in_loop(submissions)\n",
    "comments = drop_in_loop(comments)\n",
    "\n",
    "print(len(submissions), len(comments))\n",
    "\n",
    "k = 100\n",
    "sub_sample = submissions.sample(n=min(k, len(submissions)), random_state=42)\n",
    "com_sample = comments.sample(n=min(k, len(comments)), random_state=42)\n",
    "\n",
    "print(len(sub_sample), len(com_sample))\n",
    "\n",
    "sub_sample.to_csv(\"submissions_sample100.csv\", index=False)\n",
    "com_sample.to_csv(\"comments_sample100.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c48ef",
   "metadata": {},
   "source": [
    "\n",
    "#### Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff47dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_comments = pd.read_csv(r\"C:\\Users\\ftomi\\Desktop\\NLP project\\comments_test_sample.csv\")\n",
    "test_submissions = pd.read_csv(r\"C:\\Users\\ftomi\\Desktop\\NLP project\\submissions_test_sample.csv\")\n",
    "\n",
    "def finbert_evaluation(df):\n",
    "    text_og = df[\"selftext\"].astype(str).tolist()\n",
    "    text_pp = df[\"processed_text\"].astype(str).tolist()\n",
    "    probs_og = finbert_proba(text_og, device=DEVICE)\n",
    "    probs_pp = finbert_proba(text_pp, device=DEVICE)\n",
    "    argmax_og = [max(p.items(), key=lambda kv: kv[1]) for p in probs_og]\n",
    "    argmax_pp = [max(p.items(), key=lambda kv: kv[1]) for p in probs_pp]\n",
    "    df[\"selftext_finbert_label\"] = [k for k, v in argmax_og]\n",
    "    df[\"selftext_finbert_score\"] = [v for k, v in argmax_og]\n",
    "    df[\"processed_finbert_label\"] = [k for k, v in argmax_pp]\n",
    "    df[\"processed_finbert_score\"] = [v for k, v in argmax_pp]\n",
    "    return df\n",
    "\n",
    "# outputs in SAME folder as inputs\n",
    "comments_test_complete = finbert_evaluation(test_comments)\n",
    "comments_test_complete.to_csv(\"comments_test_complete.csv\", index=False)\n",
    "subs_test_complete = finbert_evaluation(test_submissions)\n",
    "subs_test_complete.to_csv(\"submissions_test_complete.csv\", index=False)\n",
    "\n",
    "comments = pd.read_csv(r\"C:\\Users\\ftomi\\Desktop\\NLP project\\comments_test_complete.csv\")\n",
    "subs = pd.read_csv(r\"C:\\Users\\ftomi\\Desktop\\NLP project\\submissions_test_complete.csv\")\n",
    "\n",
    "# helper funcs\n",
    "def find_gold_col(df):\n",
    "    for name in [\"golden_standard_score\", \"Golden standard score\", \"golden_standard\", \"Golden standard\"]:\n",
    "        if name in df.columns: \n",
    "            return name\n",
    "    raise KeyError(\"Could not find the golden-standard column.\")\n",
    "\n",
    "def add_human_label(df):\n",
    "    df = df.copy()\n",
    "    gcol = find_gold_col(df)\n",
    "    scores = pd.to_numeric(df[gcol], errors=\"coerce\")\n",
    "    mapping = {1: \"positive\", 0: \"neutral\", -1: \"negative\"}\n",
    "    df[\"human_label\"] = scores.map(mapping)\n",
    "    return df\n",
    "\n",
    "def eval_preds(df, pred_cols):\n",
    "    \"\"\"Return accuracy + confusion matrices for each predicted label column.\"\"\"\n",
    "    out = {}\n",
    "    y_true = df[\"human_label\"].str.lower()\n",
    "    mask = y_true.notna()\n",
    "    for col in pred_cols:\n",
    "        y_pred = df[col].astype(str).str.lower()\n",
    "        acc = float((y_pred[mask] == y_true[mask]).mean())\n",
    "        cm = pd.crosstab(y_true[mask], y_pred[mask], dropna=False).reindex(\n",
    "            index=[\"negative\",\"neutral\",\"positive\"], columns=[\"negative\",\"neutral\",\"positive\"], fill_value=0\n",
    "        )\n",
    "        out[col] = {\"accuracy\": acc, \"confusion\": cm}\n",
    "    return out\n",
    "\n",
    "def print_report(name, results):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    for col, r in results.items():\n",
    "        print(f\"\\n{col}  â€”  accuracy: {r['accuracy']:.3f}\")\n",
    "        print(r[\"confusion\"])\n",
    "\n",
    "# add human labels\n",
    "comments = add_human_label(comments)\n",
    "subs = add_human_label(subs)\n",
    "\n",
    "# evaluate vs FinBERT labels\n",
    "comment_results = eval_preds(\n",
    "    comments, pred_cols=[\"selftext_finbert_label\", \"processed_finbert_label\"]\n",
    ")\n",
    "subs_results = eval_preds(\n",
    "    subs, pred_cols=[\"selftext_finbert_label\", \"processed_finbert_label\"]\n",
    ")\n",
    "\n",
    "print_report(\"COMMENTS\", comment_results)\n",
    "print_report(\"SUBMISSIONS\", subs_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": ".venv",
=======
   "display_name": "Python 3",
>>>>>>> b9477ec56fd1a2be07055c26a266040fc312fd72
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.3"
=======
   "version": "3.12.3"
>>>>>>> b9477ec56fd1a2be07055c26a266040fc312fd72
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
