{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1c4b62",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c917e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Imports'''\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "from rapidfuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944d9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reading Data'''\n",
    "data_dir_comments = r\"C:\\Users\\gungo\\OneDrive\\Desktop\\stocks_comments.ndjson\"\n",
    "data_dir_sub = r\"C:\\Users\\gungo\\OneDrive\\Desktop\\stocks_submissions.ndjson\"\n",
    "df_com = pd.read_json(data_dir_comments, lines=True)\n",
    "df_sub = pd.read_json(data_dir_sub, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3db05ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reducing Data'''\n",
    "df_com_reduced = df_com[['created_utc','score','body']]\n",
    "df_sub_reduced = df_sub[['created_utc','score','selftext']]\n",
    "\n",
    "df_com_reduced = df_com_reduced.rename(columns={'body': 'text'})\n",
    "df_sub_reduced = df_sub_reduced.rename(columns={'selftext': 'text'})\n",
    "\n",
    "df_merged = pd.concat([df_com_reduced, df_sub_reduced], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51b071c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre-Processing'''\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans, tokenizes, removes stopwords, and stems text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    #text = text.lower()       Not needed for NER. Actually makes it worse\n",
    "    text = re.sub(r'&amp;#x200B;', '', text)\n",
    "    text = re.sub('&amp;', '', text) # remove some special characters from the data &amp; corresponds to &\n",
    "    text = re.sub(r'\\s+', ' ', text)  # eliminate duplicate whitespaces using regex\n",
    "    #text = re.sub(r'\\[[^]]*\\]', '', text)  # remove text in square brackets\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'\\binc\\b', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    #text = ' '.join(stemmer.stem(word) for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Preprocesses the 'body' or 'selftext' column and removes '[removed]' entries.\"\"\"\n",
    "    \n",
    "    text_col = 'text'\n",
    "\n",
    "    # Remove NaN and '[removed]' rows\n",
    "    df = df[df[text_col].notna()]\n",
    "    df = df[~df[text_col].str.contains(r'\\[removed\\]', na=False)]\n",
    "    df = df[~df[text_col].str.contains(r'\\[deleted\\]', na=False)]\n",
    "\n",
    "    # Apply text preprocessing\n",
    "    df['processed_text'] = df[text_col].apply(preprocess_text)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "pre_processed_df = preprocess(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "959bf27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1654041658</td>\n",
       "      <td>-1</td>\n",
       "      <td>Musk is a clown. He knew 50% of his followers ...</td>\n",
       "      <td>Musk clown. He knew 50% followers bots. He kne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654041696</td>\n",
       "      <td>100</td>\n",
       "      <td>What's the cumulative short loss? $50 billion ...</td>\n",
       "      <td>What's cumulative short loss? $50 billion coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1654041706</td>\n",
       "      <td>2</td>\n",
       "      <td>Quantum computing is physics, but physics isn'...</td>\n",
       "      <td>Quantum computing physics, physics business. A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654041743</td>\n",
       "      <td>62</td>\n",
       "      <td>MANGA</td>\n",
       "      <td>MANGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654041839</td>\n",
       "      <td>8</td>\n",
       "      <td>AMD?\\n\\nThey sell on the merits of their produ...</td>\n",
       "      <td>AMD? They sell merits products, open source so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1654041840</td>\n",
       "      <td>12</td>\n",
       "      <td>Highly coincidental that this drastic drop in ...</td>\n",
       "      <td>Highly coincidental drastic drop price happene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1654041851</td>\n",
       "      <td>2</td>\n",
       "      <td>Of course you can time the market, on a macro ...</td>\n",
       "      <td>Of course time market, macro basis - follow Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1654041856</td>\n",
       "      <td>1</td>\n",
       "      <td>However the issue is with the decay.  It may s...</td>\n",
       "      <td>However issue decay. It may show 100% gains hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1654041865</td>\n",
       "      <td>13</td>\n",
       "      <td>They exclude the 5% they know about.\\n\\nAnd it...</td>\n",
       "      <td>They exclude 5% know about. And matter. Advert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1654041907</td>\n",
       "      <td>11</td>\n",
       "      <td>The board dgaf what Dorsey days.</td>\n",
       "      <td>The board dgaf Dorsey days.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1654041958</td>\n",
       "      <td>1</td>\n",
       "      <td>Smh</td>\n",
       "      <td>Smh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1654041964</td>\n",
       "      <td>1</td>\n",
       "      <td>This explains why the metaverse ETF META (that...</td>\n",
       "      <td>This explains metaverse ETF META (that flat no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1654041969</td>\n",
       "      <td>2</td>\n",
       "      <td>Yea agree buying a house outright is not the b...</td>\n",
       "      <td>Yea agree buying house outright best use capital.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1654041986</td>\n",
       "      <td>7</td>\n",
       "      <td>You can't waive...jeebus...\\n\\nTwitter is in t...</td>\n",
       "      <td>You can't waive...jeebus... Twitter clear. It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1654042051</td>\n",
       "      <td>1</td>\n",
       "      <td>I've been buying oil since the mid 60s and tha...</td>\n",
       "      <td>I've buying oil since mid 60s reasoning: -Shal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1654042058</td>\n",
       "      <td>1</td>\n",
       "      <td>I work in the camping industry. And yes they'r...</td>\n",
       "      <td>I work camping industry. And yes gonna keep bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1654042079</td>\n",
       "      <td>2</td>\n",
       "      <td>Agree its a lousy time to buy a house</td>\n",
       "      <td>Agree lousy time buy house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1654042092</td>\n",
       "      <td>1</td>\n",
       "      <td>Yeah I am so good at stock picking maybe I sho...</td>\n",
       "      <td>Yeah I good stock picking maybe I gamble one t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1654042092</td>\n",
       "      <td>9</td>\n",
       "      <td>Oof</td>\n",
       "      <td>Oof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1654042100</td>\n",
       "      <td>9</td>\n",
       "      <td>He seems to already have a liquidated billions...</td>\n",
       "      <td>He seems already liquidated billions Tesla sha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc  score                                               text  \\\n",
       "0    1654041658     -1  Musk is a clown. He knew 50% of his followers ...   \n",
       "1    1654041696    100  What's the cumulative short loss? $50 billion ...   \n",
       "2    1654041706      2  Quantum computing is physics, but physics isn'...   \n",
       "3    1654041743     62                                              MANGA   \n",
       "4    1654041839      8  AMD?\\n\\nThey sell on the merits of their produ...   \n",
       "5    1654041840     12  Highly coincidental that this drastic drop in ...   \n",
       "6    1654041851      2  Of course you can time the market, on a macro ...   \n",
       "7    1654041856      1  However the issue is with the decay.  It may s...   \n",
       "8    1654041865     13  They exclude the 5% they know about.\\n\\nAnd it...   \n",
       "9    1654041907     11                   The board dgaf what Dorsey days.   \n",
       "10   1654041958      1                                                Smh   \n",
       "11   1654041964      1  This explains why the metaverse ETF META (that...   \n",
       "12   1654041969      2  Yea agree buying a house outright is not the b...   \n",
       "13   1654041986      7  You can't waive...jeebus...\\n\\nTwitter is in t...   \n",
       "14   1654042051      1  I've been buying oil since the mid 60s and tha...   \n",
       "15   1654042058      1  I work in the camping industry. And yes they'r...   \n",
       "16   1654042079      2              Agree its a lousy time to buy a house   \n",
       "17   1654042092      1  Yeah I am so good at stock picking maybe I sho...   \n",
       "18   1654042092      9                                                Oof   \n",
       "19   1654042100      9  He seems to already have a liquidated billions...   \n",
       "\n",
       "                                       processed_text  \n",
       "0   Musk clown. He knew 50% followers bots. He kne...  \n",
       "1   What's cumulative short loss? $50 billion coun...  \n",
       "2   Quantum computing physics, physics business. A...  \n",
       "3                                               MANGA  \n",
       "4   AMD? They sell merits products, open source so...  \n",
       "5   Highly coincidental drastic drop price happene...  \n",
       "6   Of course time market, macro basis - follow Th...  \n",
       "7   However issue decay. It may show 100% gains hi...  \n",
       "8   They exclude 5% know about. And matter. Advert...  \n",
       "9                         The board dgaf Dorsey days.  \n",
       "10                                                Smh  \n",
       "11  This explains metaverse ETF META (that flat no...  \n",
       "12  Yea agree buying house outright best use capital.  \n",
       "13  You can't waive...jeebus... Twitter clear. It ...  \n",
       "14  I've buying oil since mid 60s reasoning: -Shal...  \n",
       "15  I work camping industry. And yes gonna keep bu...  \n",
       "16                         Agree lousy time buy house  \n",
       "17  Yeah I good stock picking maybe I gamble one t...  \n",
       "18                                                Oof  \n",
       "19  He seems already liquidated billions Tesla sha...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Display'''\n",
    "pre_processed_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d007d",
   "metadata": {},
   "source": [
    "NAMED ENTITY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34d5b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_path = r'Top_100.csv'\n",
    "Top_100 = pd.read_csv(top100_path)\n",
    "\n",
    "Top_100.columns = [col.strip().lower() for col in Top_100.columns]\n",
    "\n",
    "# Create dictionaries for fast lookups\n",
    "ticker_to_name = dict(zip(Top_100['symbol'].str.upper(), Top_100['name']))\n",
    "valid_tickers = set(ticker_to_name.keys())\n",
    "company_names = [name.lower() for name in ticker_to_name.values()]\n",
    "name_to_ticker = {name.lower(): symbol for symbol, name in ticker_to_name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42dbee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#to be ran per comment\n",
    "def extract_ner_entities(model, text, similarity_threshold=90):\n",
    "    \n",
    "    BLACKLIST = {'ev', 'covid', 'etf', 'nyse', 'sec', 'spac', 'fda', 'treasury', 'covid-19', 'rrsp', 'tfsa','fed'}\n",
    "    doc = model(text)\n",
    "    detected_companies = []\n",
    "\n",
    "    #Detect companies via spaCy NER\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" and ent.text.lower() not in BLACKLIST:\n",
    "            org_name = ent.text.strip()\n",
    "            # Fuzzy match against official company names from csv file\n",
    "            match, score, _ = process.extractOne(org_name.lower(), company_names, scorer=fuzz.token_sort_ratio)\n",
    "            if score >= similarity_threshold:\n",
    "                matched_ticker = name_to_ticker[match]\n",
    "                canonical_name = ticker_to_name[matched_ticker]\n",
    "                detected_companies.append(canonical_name)\n",
    "            else:\n",
    "                #keeps companies not in csv file maybe delete later\n",
    "                detected_companies.append(org_name)\n",
    "\n",
    "    # --- Match stock tickers in text ---\n",
    "    for token in doc:\n",
    "        token_text = token.text.strip()\n",
    "\n",
    "        # Handle tickers with $ prefix, e.g. $AAPL\n",
    "        if token_text.startswith(\"$\"):\n",
    "            token_text = token_text[1:]\n",
    "\n",
    "        # Check if it’s a valid ticker symbol\n",
    "        if token_text in valid_tickers:\n",
    "            company_name = ticker_to_name.get(token_text)\n",
    "            detected_companies.append(company_name)\n",
    "\n",
    "\n",
    "    return list(set(detected_companies))\n",
    "\n",
    "\n",
    "def get_dict_top_companies(dataset, column_name, top_companies=10):\n",
    "    company_counter = dict()\n",
    "    for companies in dataset[column_name]:\n",
    "        for company in companies:\n",
    "            if company in company_counter:  \n",
    "                company_counter[company] += 1\n",
    "            else:\n",
    "                company_counter[company] = 1\n",
    "    sorted_dict = dict(sorted(company_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "    top = dict()\n",
    "    for company, count in list(sorted_dict.items())[:top_companies]:\n",
    "        top[company] = count\n",
    "\n",
    "    return top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b5e7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure every cell is a string (NaN -> \"\")\n",
    "pre_processed_df['processed_text'] = pre_processed_df['processed_text'].fillna(\"\").astype(str)\n",
    "\n",
    "# Safe wrapper so extract_ner_entities always receives a string\n",
    "def safe_extract(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    return extract_ner_entities(nlp, text, similarity_threshold=60)\n",
    "\n",
    "pre_processed_df[\"Companies\"] = pre_processed_df['processed_text'].apply(safe_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f5d908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top companies: {'Apple': 9784, 'Tesla': 8876, 'Amazon': 6926}\n",
      "Amazon: 6926 rows saved to companies_csv\\Amazon_company_data.csv\n",
      "Tesla: 8876 rows saved to companies_csv\\Tesla_company_data.csv\n",
      "Apple: 9784 rows saved to companies_csv\\Apple_company_data.csv\n",
      "\n",
      "Preview of each company's dataframe:\n",
      "\n",
      "=== AMAZON (6926 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>Companies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1654044916</td>\n",
       "      <td>3</td>\n",
       "      <td>Split was announced march 9. Price was about $...</td>\n",
       "      <td>Split announced march 9. Price $2800. I would ...</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1654044952</td>\n",
       "      <td>4</td>\n",
       "      <td>Best time to grab Amazon for example, in more ...</td>\n",
       "      <td>Best time grab Amazon example, recent times, d...</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1654045467</td>\n",
       "      <td>4</td>\n",
       "      <td>I’ve bought Amazon the last week or so while i...</td>\n",
       "      <td>I’ve bought Amazon last week gonna keep adding...</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     created_utc  score                                               text  \\\n",
       "106   1654044916      3  Split was announced march 9. Price was about $...   \n",
       "108   1654044952      4  Best time to grab Amazon for example, in more ...   \n",
       "125   1654045467      4  I’ve bought Amazon the last week or so while i...   \n",
       "\n",
       "                                        processed_text Companies  \n",
       "106  Split announced march 9. Price $2800. I would ...    Amazon  \n",
       "108  Best time grab Amazon example, recent times, d...    Amazon  \n",
       "125  I’ve bought Amazon last week gonna keep adding...    Amazon  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== TESLA (8876 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>Companies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654041696</td>\n",
       "      <td>100</td>\n",
       "      <td>What's the cumulative short loss? $50 billion ...</td>\n",
       "      <td>What's cumulative short loss? $50 billion coun...</td>\n",
       "      <td>Tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1654042100</td>\n",
       "      <td>9</td>\n",
       "      <td>He seems to already have a liquidated billions...</td>\n",
       "      <td>He seems already liquidated billions Tesla sha...</td>\n",
       "      <td>Tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1654043939</td>\n",
       "      <td>-3</td>\n",
       "      <td>https://www.theverge.com/2022/5/10/23065061/te...</td>\n",
       "      <td>Tesla won’t beat Q2 earning estimates. May Elo...</td>\n",
       "      <td>Tesla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc  score                                               text  \\\n",
       "1    1654041696    100  What's the cumulative short loss? $50 billion ...   \n",
       "19   1654042100      9  He seems to already have a liquidated billions...   \n",
       "70   1654043939     -3  https://www.theverge.com/2022/5/10/23065061/te...   \n",
       "\n",
       "                                       processed_text Companies  \n",
       "1   What's cumulative short loss? $50 billion coun...     Tesla  \n",
       "19  He seems already liquidated billions Tesla sha...     Tesla  \n",
       "70  Tesla won’t beat Q2 earning estimates. May Elo...     Tesla  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== APPLE (9784 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>Companies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654041839</td>\n",
       "      <td>8</td>\n",
       "      <td>AMD?\\n\\nThey sell on the merits of their produ...</td>\n",
       "      <td>AMD? They sell merits products, open source so...</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>1654048857</td>\n",
       "      <td>31</td>\n",
       "      <td>The thing that keeps me bullish on AAPL's sust...</td>\n",
       "      <td>The thing keeps bullish AAPL's sustained growt...</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1654049132</td>\n",
       "      <td>11</td>\n",
       "      <td>AAPL is on sale right now</td>\n",
       "      <td>AAPL sale right</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     created_utc  score                                               text  \\\n",
       "4     1654041839      8  AMD?\\n\\nThey sell on the merits of their produ...   \n",
       "223   1654048857     31  The thing that keeps me bullish on AAPL's sust...   \n",
       "235   1654049132     11                          AAPL is on sale right now   \n",
       "\n",
       "                                        processed_text Companies  \n",
       "4    AMD? They sell merits products, open source so...     Apple  \n",
       "223  The thing keeps bullish AAPL's sustained growt...     Apple  \n",
       "235                                    AAPL sale right     Apple  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Saved filtered_top_companies.csv\n"
     ]
    }
   ],
   "source": [
    "top_dict = get_dict_top_companies(pre_processed_df, \"Companies\", 3)\n",
    "top_set = set(top_dict.keys())\n",
    "print(f\"Top companies: {top_dict}\")\n",
    "\n",
    "masked = pre_processed_df['Companies'].apply(lambda lst: bool(top_set.intersection(lst)))\n",
    "filtered_df = pre_processed_df[masked].copy()\n",
    "\n",
    "exploded_sub = filtered_df.explode('Companies')\n",
    "\n",
    "# Keep only rows for top companies \n",
    "exploded_sub = exploded_sub[exploded_sub['Companies'].isin(top_set)].copy()\n",
    "\n",
    "# Create separate dataframes for each top company \n",
    "dfs_by_company = {}\n",
    "os.makedirs(\"companies_csv\", exist_ok=True)\n",
    "\n",
    "for company in top_set:\n",
    "    dfs_by_company[company] = exploded_sub[exploded_sub['Companies'] == company].copy()\n",
    "    file_path = os.path.join(\"companies_csv\", f\"{company}_company_data.csv\")\n",
    "    dfs_by_company[company].to_csv(file_path, index=False)\n",
    "    print(f\"{company}: {len(dfs_by_company[company])} rows saved to {file_path}\")\n",
    "\n",
    "print(\"\\nPreview of each company's dataframe:\\n\")\n",
    "for company, df_company in dfs_by_company.items():\n",
    "    print(f\"=== {company.upper()} ({len(df_company)} rows) ===\")\n",
    "    display(df_company.head(3))   \n",
    "    print(\"\\n\")\n",
    "\n",
    "filtered_df.to_csv(\"filtered_top_companies.csv\", index=False)  #not really needed, just the dataframe with all top companies submissions\n",
    "print(\"Saved filtered_top_companies.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a029a389",
   "metadata": {},
   "source": [
    "## FinBERT Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b848ec",
   "metadata": {},
   "source": [
    "\n",
    "#### Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb213738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"    # avoid TF backend import\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"  # avoid Flax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093f163",
   "metadata": {},
   "source": [
    "\n",
    "#### FinBERT Model Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f76847b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gungo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gungo\\.cache\\huggingface\\hub\\models--ProsusAI--finbert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FinBERT on cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL = \"ProsusAI/finbert\"\n",
    "\n",
    "# Only install the model on CPU if it isn't yet\n",
    "if \"model\" not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    DEVICE = \"cpu\"\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded FinBERT on {DEVICE}.\")\n",
    "else:\n",
    "    new_device = \"cpu\"\n",
    "    if str(next(model.parameters()).device) != new_device:\n",
    "        model.to(new_device).eval()\n",
    "        DEVICE = new_device\n",
    "        print(f\"Moved model to {DEVICE}.\")\n",
    "    else:\n",
    "        print(f\"Model already loaded to {str(next(model.parameters()).device)}; skipping.\")\n",
    "\n",
    "id2label = model.config.id2label  \n",
    "# id2abel --> {0:'positive',1:'negative',2:'neutral'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589da652",
   "metadata": {},
   "source": [
    "\n",
    "#### Function Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b34d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def finbert_proba(texts, batch_size=32, device=DEVICE, max_length=256):\n",
    "    \"\"\"\n",
    "    Return a list of dicts of probabilities: [{'negative': p, 'neutral': p, 'positive': p}, ...]\n",
    "    \"\"\"\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            ).to(device)\n",
    "            logits = model(**enc).logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().tolist()\n",
    "            for p in probs:\n",
    "                # map indices -> labels dict\n",
    "                out = {id2label[j]: float(p[j]) for j in range(len(p))}\n",
    "                # ensure all three keys exist\n",
    "                for k in (\"negative\", \"neutral\", \"positive\"):\n",
    "                    out.setdefault(k, 0.0)\n",
    "                all_probs.append(out)\n",
    "    return all_probs\n",
    "\n",
    "def top2_margin(p: dict) -> float:\n",
    "    \"\"\"\n",
    "    Return max_prob - second_prob in [0,1].\n",
    "    \"\"\"\n",
    "    vals = sorted([p[\"negative\"], p[\"neutral\"], p[\"positive\"]], reverse=True)\n",
    "    return float(vals[0] - vals[1])\n",
    "\n",
    "def sentiment_score(texts):\n",
    "    \"\"\"\n",
    "    Return continuous score per text in [-1,1]: (p(pos) - p(neg)). \n",
    "    Appliy damping based on neutrality and top 2 margin\n",
    "    \"\"\"\n",
    "    probs = finbert_proba(texts)\n",
    "    scores = []\n",
    "    for p in probs:\n",
    "        base = (p[\"positive\"] - p[\"negative\"])           \n",
    "        neutral_damp = (1.0 - p[\"neutral\"])              \n",
    "        margin_damp = top2_margin(p)                           \n",
    "        scores.append(base * neutral_damp * margin_damp)\n",
    "    return scores\n",
    "\n",
    "# tanh with 30D window\n",
    "def tanh_scale_series(scores, timestamps=None, percentile=95, min_periods=14):\n",
    "    \"\"\"\n",
    "    30D rolling P95 tanh scaling.\n",
    "    Returns\n",
    "      w: np.ndarray in [-1, 1] (tanh-scaled, sign preserved)\n",
    "      k_used: np.ndarray of per-row k values (rolling P95 of |score|)\n",
    "    \"\"\"\n",
    "    s = np.asarray(scores, dtype=float)\n",
    "    if s.size == 0:\n",
    "        return s.astype(float), np.array([], dtype=float)\n",
    "    s = np.nan_to_num(s, nan=0.0)\n",
    "\n",
    "    if timestamps is not None:\n",
    "        ts = pd.to_datetime(pd.Series(timestamps), utc=True, errors=\"coerce\")\n",
    "        abs_s = pd.Series(np.abs(s), index=ts).sort_index()\n",
    "        roll_p = abs_s.rolling(\"30D\", min_periods=min_periods).quantile(percentile/100.0)\n",
    "        g_p = abs_s.quantile(percentile/100.0) if len(abs_s) else 1.0\n",
    "        k_series = roll_p.fillna(g_p).clip(lower=1.0)\n",
    "        # align per-row k back to original order (by timestamps)\n",
    "        k_used = k_series.reindex(ts).to_numpy()\n",
    "        # fallback for NaT rows\n",
    "        k_used = np.where(np.isfinite(k_used), k_used, max(g_p, 1.0))\n",
    "    else:\n",
    "        # global P95\n",
    "        g_p = np.percentile(np.abs(s), percentile) if s.size else 1.0\n",
    "        k_used = np.full_like(s, fill_value=max(g_p, 1.0), dtype=float)\n",
    "\n",
    "    w = np.tanh(s / k_used)\n",
    "    return w.astype(float), k_used.astype(float)\n",
    "\n",
    "def score_multiplier(raw_sentiment_scores, reddit_scores, percentile=95):\n",
    "    \"\"\"\n",
    "    returns: list of crowd-adjusted scores in [-1, 1]\n",
    "    \"\"\"\n",
    "    rs = np.asarray(raw_sentiment_scores, dtype=float)\n",
    "    w, k_used = tanh_scale_series(reddit_scores, percentile=percentile)\n",
    "    # elementwise combine to not multiply by raw reddit_scores again\n",
    "    return (rs * w).tolist(), k_used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21466135",
   "metadata": {},
   "source": [
    "\n",
    "#### Running the Model on Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0426c",
   "metadata": {},
   "source": [
    "Data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78acb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# proper, multiple datasets lines below\n",
    "file_locations ={\"Amazon\" : r\"companies_csv\\Amazon_company_data.csv\", \n",
    "                 \"Apple\" : r\"companies_csv\\Apple_company_data.csv\", \n",
    "                 \"Tesla\" : r\"companies_csv\\Tesla_company_data.csv\"} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd871f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# singular test line below\n",
    "# file_locations = {\"Amazon\" : r\"C:\\Users\\ftomi\\Desktop\\NLP project\\Amazon_submissions.csv\"}\n",
    "\n",
    "#using dfs_by_company -> {\"Amazon\" : df_amazon}\n",
    "\n",
    "for fl in file_locations:\n",
    "#for company, df_company in dfs_by_company.items():\n",
    "\n",
    "    df = pd.read_csv(file_locations[fl])\n",
    "\n",
    "    dates = []\n",
    "    texts = []\n",
    "    upvotes = [] \n",
    "    scaled_sentiment_scores = []\n",
    "    raw_sentiment_scores = []\n",
    "\n",
    "    data = df.to_dict('index')\n",
    "    for i in range(len(data)):\n",
    "        texts.append(data[i]['processed_text'].lower())\n",
    "        dates.append(datetime.fromtimestamp(data[i]['created_utc'], tz=timezone.utc))\n",
    "        upvotes.append(data[i]['score'])\n",
    "\n",
    "    probs = finbert_proba(texts)\n",
    "    raw_sentiment_scores = sentiment_score(texts)\n",
    "    scaled_sentiment_scores, k_used = score_multiplier(sentiment_score(texts.lower()), upvotes)\n",
    "    \n",
    "    # save as dataframe\n",
    "    sentiment_df = pd.DataFrame(dict(scaled_sentiment = scaled_sentiment_scores, created_utc = dates))\n",
    "    sentiment_df.to_csv(f\"{fl}_sentiment_data.csv\", index=False)\n",
    "\n",
    "    #show and save figure\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(dates, raw_sentiment_scores, label = \"raw\",  linestyle=\":\", linewidth=0.8)\n",
    "    plt.plot(dates, scaled_sentiment_scores, label = \"scaled by reddit score\")\n",
    "    plt.title(f\"Sentiment Towards {fl} on r/stocks\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Total Sentiment\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{fl}_plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cabc83",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a363ea",
   "metadata": {},
   "source": [
    "\n",
    "#### Sample Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb68cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon -> kept 14 rows from Amazon_company_data.csv\n",
      "Apple -> kept 20 rows from Apple_company_data.csv\n",
      "Tesla -> kept 20 rows from Tesla_company_data.csv\n",
      "\n",
      "Per-file rows kept:\n",
      "__source\n",
      "Apple     20\n",
      "Tesla     20\n",
      "Amazon    14\n",
      "Name: count, dtype: int64\n",
      "Wrote: FinBERT_TEST_SET.csv | rows: 54\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "random.seed(42) \n",
    "\n",
    "p = 0.002           \n",
    "\n",
    "samples = []\n",
    "\n",
    "for name, path in file_locations.items():   # dict: key=name, value=path\n",
    "    df = pd.read_csv(path, skiprows=lambda i: (i > 0) and (random.random() > p))\n",
    "    df[\"__source\"] = name  # so we can see contributions\n",
    "    print(f\"{name} -> kept {len(df):,} rows from {os.path.basename(path)}\")\n",
    "    samples.append(df)\n",
    "\n",
    "combined = pd.concat(samples, ignore_index=True)\n",
    "print(\"\\nPer-file rows kept:\")\n",
    "print(combined[\"__source\"].value_counts())\n",
    "\n",
    "combined.to_csv(\"FinBERT_TEST_SET.csv\", index=False)\n",
    "print(\"Wrote: FinBERT_TEST_SET.csv | rows:\", len(combined))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a0eda",
   "metadata": {},
   "source": [
    "\n",
    "#### Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dea2d46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMMENTS ===\n",
      "\n",
      "selftext_finbert_label  —  accuracy: 0.538\n",
      "selftext_finbert_label  negative  neutral  positive\n",
      "human_label                                        \n",
      "negative                       0        3         1\n",
      "neutral                        3       13         0\n",
      "positive                       0        5         1\n",
      "\n",
      "processed_finbert_label  —  accuracy: 0.500\n",
      "processed_finbert_label  negative  neutral  positive\n",
      "human_label                                         \n",
      "negative                        0        3         1\n",
      "neutral                         3       13         0\n",
      "positive                        1        5         0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using the file 'test_sample_annotated.csv' which is a redacted, human-annotated version of 'FinBERT_TEST_SET.csv'.\n",
    "test_sample_annotated = pd.read_csv(r\"test_sample_annotated.csv\")\n",
    "\n",
    "def finbert_evaluation(df):\n",
    "    text_og = df[\"text\"].astype(str).str.lower().tolist()\n",
    "    text_pp = df[\"processed_text\"].astype(str).str.lower().tolist()\n",
    "    probs_og = finbert_proba(text_og, device=DEVICE)\n",
    "    probs_pp = finbert_proba(text_pp, device=DEVICE)\n",
    "    argmax_og = [max(p.items(), key=lambda kv: kv[1]) for p in probs_og]\n",
    "    argmax_pp = [max(p.items(), key=lambda kv: kv[1]) for p in probs_pp]\n",
    "    df[\"selftext_finbert_label\"] = [k for k, v in argmax_og]\n",
    "    df[\"selftext_finbert_score\"] = [v for k, v in argmax_og]\n",
    "    df[\"processed_finbert_label\"] = [k for k, v in argmax_pp]\n",
    "    df[\"processed_finbert_score\"] = [v for k, v in argmax_pp]\n",
    "    return df\n",
    "\n",
    "# outputs in SAME folder as inputs\n",
    "test_complete = finbert_evaluation(test_sample_annotated)\n",
    "test_complete.to_csv(\"test_sample_complete.csv\", index=False)\n",
    "\n",
    "# helper funcs \n",
    "def find_gold_col(df):\n",
    "    for name in [\"gold_score\"]:\n",
    "        if name in df.columns: \n",
    "            return name\n",
    "    raise KeyError(\"Could not find the golden-standard column.\")\n",
    "\n",
    "def add_human_label(df):\n",
    "    df = df.copy()\n",
    "    gcol = find_gold_col(df)\n",
    "    scores = pd.to_numeric(df[gcol], errors=\"coerce\")\n",
    "    mapping = {1: \"positive\", 0: \"neutral\", -1: \"negative\"}\n",
    "    df[\"human_label\"] = scores.map(mapping)\n",
    "    return df\n",
    "\n",
    "def eval_preds(df, pred_cols):\n",
    "    \"\"\"Return accuracy + confusion matrices for each predicted label column.\"\"\"\n",
    "    out = {}\n",
    "    y_true = df[\"human_label\"].str.lower()\n",
    "    mask = y_true.notna()\n",
    "    for col in pred_cols:\n",
    "        y_pred = df[col].astype(str).str.lower()\n",
    "        acc = float((y_pred[mask] == y_true[mask]).mean())\n",
    "        cm = pd.crosstab(y_true[mask], y_pred[mask], dropna=False).reindex(\n",
    "            index=[\"negative\",\"neutral\",\"positive\"], columns=[\"negative\",\"neutral\",\"positive\"], fill_value=0\n",
    "        )\n",
    "        out[col] = {\"accuracy\": acc, \"confusion\": cm}\n",
    "    return out\n",
    "\n",
    "def print_report(name, results):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    for col, r in results.items():\n",
    "        print(f\"\\n{col}  —  accuracy: {r['accuracy']:.3f}\")\n",
    "        print(r[\"confusion\"])\n",
    "\n",
    "# add human-labels\n",
    "comments = add_human_label(test_sample_annotated)\n",
    "\n",
    "# evaluate vs FinBERT labels (adjust column names if yours differ)\n",
    "comment_results = eval_preds(\n",
    "    comments, pred_cols=[\"selftext_finbert_label\", \"processed_finbert_label\"]\n",
    ")\n",
    "\n",
    "print_report(\"COMMENTS\", comment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea129bc4",
   "metadata": {},
   "source": [
    "## Sentiment-Stock Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#CONFIG\n",
    "START_noNAN = \"2022-05-30\"   # start a day early to avoid first-return NaN\n",
    "START     = \"2022-06-01\"\n",
    "END       = \"2022-12-31\"\n",
    "LAGS      = range(-3, 4)   # -3 to +3\n",
    "\n",
    "# Each df must have columns: created_utc, scaled_sentiment\n",
    "'''Connect with FinBERT df's'''\n",
    "sentiment_daily_by_company = {\n",
    "    #\"AMZN\": amzn_sent_df,   \n",
    "    #\"AAPL\": aapl_sent_df,   \n",
    "    #\"TSLA\": tsla_sent_df,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_returns(ticker, start=START_noNAN, end=END):\n",
    "\n",
    "    data = yf.download(ticker, start=start, end=end)\n",
    "    # Flatten possible MultiIndex columns\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        data.columns = ['_'.join(c).strip() if isinstance(c, tuple) else c for c in data.columns]\n",
    "\n",
    "    # Find the \"Close\" column \n",
    "    close_col = [c for c in data.columns if 'Close' in c and not 'Adj' in c][0]\n",
    "\n",
    "    # Daily returns and slice actual window\n",
    "    data['daily_return'] = data[close_col].pct_change()\n",
    "    data = data.loc[START:END]\n",
    "\n",
    "    # Make index tz-naive\n",
    "    data.index = pd.to_datetime(data.index).tz_localize(None)\n",
    "    \n",
    "    return data[['daily_return']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2313e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_sentiment_daily(df):\n",
    "    \"\"\"Takes a per-company sentiment df with columns created_utc, scaled_sentiment and returns daily mean for every day in range.\"\"\"\n",
    "    sr = df.copy()\n",
    "    # Parse datetime (tz-aware), then drop tz to align with yfinance\n",
    "\n",
    "    sr['date'] = pd.to_datetime(sr['created_utc'], utc=True, errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "    # Force numeric and clip to [-1,1]\n",
    "    sr['scaled_sentiment'] = pd.to_numeric(sr['scaled_sentiment'], errors='coerce').clip(-1, 1)\n",
    "    \n",
    "    # Group to daily means\n",
    "    daily = (sr\n",
    "             .set_index('date')\n",
    "             .groupby(pd.Grouper(freq='D'))['scaled_sentiment']\n",
    "             .mean()\n",
    "             .to_frame('mean_sentiment')\n",
    "             .loc[START:END])\n",
    "    \n",
    "    return daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_company(ticker, sent_df):\n",
    "    print(f\"\\n=== {ticker} ===\")\n",
    "    \n",
    "    returns = get_stock_returns(ticker, start=START_noNAN, end=END)\n",
    "    sentiment_daily = prep_sentiment_daily(sent_df)\n",
    "\n",
    "    # Build correlation-ready dataset (drop missing on either side)\n",
    "    combined_corr = (returns\n",
    "                     .join(sentiment_daily, how='inner')\n",
    "                     .dropna(subset=['daily_return', 'mean_sentiment']))\n",
    "\n",
    "    stock_days     = len(returns.dropna(subset=['daily_return']))\n",
    "    sentiment_days = len(sentiment_daily.dropna(subset=['mean_sentiment']))\n",
    "    analyzed_days  = len(combined_corr)\n",
    "    print(f\"Trading days in window: {stock_days}\")\n",
    "    print(f\"Days with sentiment: {sentiment_days}\")\n",
    "    print(f\"Days analyzed: {analyzed_days}\")\n",
    "\n",
    "\n",
    "    # Lag test \n",
    "    print(\"\\nLag correlation (mean_sentiment shifted by lag):\")\n",
    "    for lag in LAGS:\n",
    "        p = combined_corr['daily_return'].corr(combined_corr['mean_sentiment'].shift(lag), method='pearson')\n",
    "        s = combined_corr['daily_return'].corr(combined_corr['mean_sentiment'].shift(lag), method='spearman')\n",
    "        direction = (\"sentiment follows price\" if lag < 0 else \"same day\" if lag == 0 else \"sentiment leads price\")\n",
    "        print(f\"Lag {lag:+} days ({direction:23s}) | Pearson: {p: .3f} | Spearman: {s: .3f}\")\n",
    "\n",
    "    # Plotting dataset (plot for all trading days, interpolate for days missing sentiment)\n",
    "    sentiment_plot = sentiment_daily.copy()\n",
    "    sentiment_plot['mean_sentiment'] = sentiment_plot['mean_sentiment'].interpolate(limit_direction='both')\n",
    "\n",
    "    combined_plot = returns.join(sentiment_plot, how='left')\n",
    "\n",
    "    # Normalize for plotting\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    combined_plot['return_norm']    = scaler.fit_transform(combined_plot[['daily_return']])\n",
    "    combined_plot['sentiment_norm'] = scaler.fit_transform(combined_plot[['mean_sentiment']])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(combined_plot.index, combined_plot['return_norm'],    label=f'{ticker} Daily Return (normalized)', linewidth=2)\n",
    "    plt.plot(combined_plot.index, combined_plot['sentiment_norm'], label=f'{ticker} Sentiment (normalized, interpolated)', linewidth=2, alpha=0.85)\n",
    "    plt.title(f'{ticker}: Sentiment vs Daily Returns ({START} to {END})', fontsize=14)\n",
    "    plt.xlabel('Date'); plt.ylabel('Normalized scale (-1 to 1)')\n",
    "    plt.legend(); plt.grid(True, linestyle='--', alpha=0.6); plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582aeab",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb997bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker, sent_df in sentiment_daily_by_company.items():\n",
    "    analyze_company(ticker, sent_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
