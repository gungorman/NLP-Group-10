{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1c4b62",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c917e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Imports'''\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "from rapidfuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944d9c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evaga\\AppData\\Local\\Temp\\ipykernel_35176\\962546971.py:4: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_com = pd.read_json(data_dir_comments, lines=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m data_dir_comments \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgungo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mstocks_comments.ndjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m data_dir_sub \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgungo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mstocks_submissions.ndjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m df_com \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir_comments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df_sub \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(data_dir_sub, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\evaga\\anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py:804\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evaga\\anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py:1012\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m   1011\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1012\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\evaga\\anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py:1040\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1038\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1040\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\evaga\\anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py:1173\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\evaga\\anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py:1366\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1362\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1366\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1369\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1372\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "'''Reading Data'''\n",
    "data_dir_comments = r\"C:\\Users\\gungo\\OneDrive\\Desktop\\stocks_comments.ndjson\"\n",
    "data_dir_sub = r\"C:\\Users\\gungo\\OneDrive\\Desktop\\stocks_submissions.ndjson\"\n",
    "df_com = pd.read_json(data_dir_comments, lines=True)\n",
    "df_sub = pd.read_json(data_dir_sub, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db05ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reducing Data'''\n",
    "df_com_reduced = df_com[['created_utc','score','body']]\n",
    "df_sub_reduced = df_sub[['created_utc','score','selftext']]\n",
    "\n",
    "# Rename columns\n",
    "df_com_reduced = df_com_reduced.rename(columns={'body': 'text'})\n",
    "df_sub_reduced = df_sub_reduced.rename(columns={'selftext': 'text'})\n",
    "\n",
    "# Concatenate\n",
    "df_merged = pd.concat([df_com_reduced, df_sub_reduced], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b071c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre-Processing'''\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans, tokenizes, removes stopwords, and stems text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    #text = text.lower()       Not needed for NER. Actually makes it worse\n",
    "    text = re.sub(r'&amp;#x200B;', '', text)\n",
    "    text = re.sub('&amp;', '', text) # remove some special characters from the data &amp; corresponds to &\n",
    "    text = re.sub(r'\\s+', ' ', text)  # eliminate duplicate whitespaces using regex\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', text)  # remove text in square brackets\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'\\binc\\b', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    #text = ' '.join(stemmer.stem(word) for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Preprocesses the 'body' or 'selftext' column and removes '[removed]' entries.\"\"\"\n",
    "    \n",
    "    text_col = 'text'\n",
    "\n",
    "    # Remove NaN and '[removed]' rows\n",
    "    df = df[df[text_col].notna()]\n",
    "    df = df[~df[text_col].str.contains(r'\\[removed\\]', na=False)]\n",
    "    df = df[~df[text_col].str.contains(r'\\[deleted\\]', na=False)]\n",
    "\n",
    "    # Apply text preprocessing\n",
    "    df['processed_text'] = df[text_col].apply(preprocess_text)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "pre_processed_df = preprocess(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bf27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1654041658</td>\n",
       "      <td>-1</td>\n",
       "      <td>Musk is a clown. He knew 50% of his followers ...</td>\n",
       "      <td>musk clown. knew 50% followers bots. knew twit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654041696</td>\n",
       "      <td>100</td>\n",
       "      <td>What's the cumulative short loss? $50 billion ...</td>\n",
       "      <td>what's cumulative short loss? $50 billion coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1654041706</td>\n",
       "      <td>2</td>\n",
       "      <td>Quantum computing is physics, but physics isn'...</td>\n",
       "      <td>quantum computing physics, physics business. p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654041743</td>\n",
       "      <td>62</td>\n",
       "      <td>MANGA</td>\n",
       "      <td>manga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654041839</td>\n",
       "      <td>8</td>\n",
       "      <td>AMD?\\n\\nThey sell on the merits of their produ...</td>\n",
       "      <td>amd? sell merits products, open source softwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1654041840</td>\n",
       "      <td>12</td>\n",
       "      <td>Highly coincidental that this drastic drop in ...</td>\n",
       "      <td>highly coincidental drastic drop price happene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1654041851</td>\n",
       "      <td>2</td>\n",
       "      <td>Of course you can time the market, on a macro ...</td>\n",
       "      <td>course time market, macro basis - follow fed. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1654041856</td>\n",
       "      <td>1</td>\n",
       "      <td>However the issue is with the decay.  It may s...</td>\n",
       "      <td>however issue decay. may show 100% gains hits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1654041865</td>\n",
       "      <td>13</td>\n",
       "      <td>They exclude the 5% they know about.\\n\\nAnd it...</td>\n",
       "      <td>exclude 5% know about. matter. advertisers get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1654041907</td>\n",
       "      <td>11</td>\n",
       "      <td>The board dgaf what Dorsey days.</td>\n",
       "      <td>board dgaf dorsey days.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1654041958</td>\n",
       "      <td>1</td>\n",
       "      <td>Smh</td>\n",
       "      <td>smh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1654041964</td>\n",
       "      <td>1</td>\n",
       "      <td>This explains why the metaverse ETF META (that...</td>\n",
       "      <td>explains metaverse etf meta (that flat novembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1654041969</td>\n",
       "      <td>2</td>\n",
       "      <td>Yea agree buying a house outright is not the b...</td>\n",
       "      <td>yea agree buying house outright best use capital.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1654041986</td>\n",
       "      <td>7</td>\n",
       "      <td>You can't waive...jeebus...\\n\\nTwitter is in t...</td>\n",
       "      <td>can't waive...jeebus... twitter clear. disclai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1654042051</td>\n",
       "      <td>1</td>\n",
       "      <td>I've been buying oil since the mid 60s and tha...</td>\n",
       "      <td>buying oil since mid 60s reasoning: -shale ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1654042058</td>\n",
       "      <td>1</td>\n",
       "      <td>I work in the camping industry. And yes they'r...</td>\n",
       "      <td>work camping industry. yes gonna keep buying r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1654042079</td>\n",
       "      <td>2</td>\n",
       "      <td>Agree its a lousy time to buy a house</td>\n",
       "      <td>agree lousy time buy house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1654042092</td>\n",
       "      <td>1</td>\n",
       "      <td>Yeah I am so good at stock picking maybe I sho...</td>\n",
       "      <td>yeah good stock picking maybe gamble one time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1654042092</td>\n",
       "      <td>9</td>\n",
       "      <td>Oof</td>\n",
       "      <td>oof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1654042100</td>\n",
       "      <td>9</td>\n",
       "      <td>He seems to already have a liquidated billions...</td>\n",
       "      <td>seems already liquidated billions tesla shares...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc  score                                               text  \\\n",
       "0    1654041658     -1  Musk is a clown. He knew 50% of his followers ...   \n",
       "1    1654041696    100  What's the cumulative short loss? $50 billion ...   \n",
       "2    1654041706      2  Quantum computing is physics, but physics isn'...   \n",
       "3    1654041743     62                                              MANGA   \n",
       "4    1654041839      8  AMD?\\n\\nThey sell on the merits of their produ...   \n",
       "5    1654041840     12  Highly coincidental that this drastic drop in ...   \n",
       "6    1654041851      2  Of course you can time the market, on a macro ...   \n",
       "7    1654041856      1  However the issue is with the decay.  It may s...   \n",
       "8    1654041865     13  They exclude the 5% they know about.\\n\\nAnd it...   \n",
       "9    1654041907     11                   The board dgaf what Dorsey days.   \n",
       "10   1654041958      1                                                Smh   \n",
       "11   1654041964      1  This explains why the metaverse ETF META (that...   \n",
       "12   1654041969      2  Yea agree buying a house outright is not the b...   \n",
       "13   1654041986      7  You can't waive...jeebus...\\n\\nTwitter is in t...   \n",
       "14   1654042051      1  I've been buying oil since the mid 60s and tha...   \n",
       "15   1654042058      1  I work in the camping industry. And yes they'r...   \n",
       "16   1654042079      2              Agree its a lousy time to buy a house   \n",
       "17   1654042092      1  Yeah I am so good at stock picking maybe I sho...   \n",
       "18   1654042092      9                                                Oof   \n",
       "19   1654042100      9  He seems to already have a liquidated billions...   \n",
       "\n",
       "                                       processed_text  \n",
       "0   musk clown. knew 50% followers bots. knew twit...  \n",
       "1   what's cumulative short loss? $50 billion coun...  \n",
       "2   quantum computing physics, physics business. p...  \n",
       "3                                               manga  \n",
       "4   amd? sell merits products, open source softwar...  \n",
       "5   highly coincidental drastic drop price happene...  \n",
       "6   course time market, macro basis - follow fed. ...  \n",
       "7   however issue decay. may show 100% gains hits ...  \n",
       "8   exclude 5% know about. matter. advertisers get...  \n",
       "9                             board dgaf dorsey days.  \n",
       "10                                                smh  \n",
       "11  explains metaverse etf meta (that flat novembe...  \n",
       "12  yea agree buying house outright best use capital.  \n",
       "13  can't waive...jeebus... twitter clear. disclai...  \n",
       "14  buying oil since mid 60s reasoning: -shale ove...  \n",
       "15  work camping industry. yes gonna keep buying r...  \n",
       "16                         agree lousy time buy house  \n",
       "17  yeah good stock picking maybe gamble one time ...  \n",
       "18                                                oof  \n",
       "19  seems already liquidated billions tesla shares...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Display'''\n",
    "pre_processed_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d007d",
   "metadata": {},
   "source": [
    "NAMED ENTITY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_path = r'C:\\Users\\gungo\\OneDrive\\Dokumente\\GitHub\\NLP-Group-10\\Top_100.csv'\n",
    "Top_100 = pd.read_csv(top100_path, encoding='latin1')\n",
    "\n",
    "Top_100.columns = [col.strip().lower() for col in Top_100.columns]\n",
    "\n",
    "# Create dictionaries for fast lookups\n",
    "ticker_to_name = dict(zip(Top_100['symbol'].str.upper(), Top_100['name']))\n",
    "valid_tickers = set(ticker_to_name.keys())\n",
    "company_names = [name.lower() for name in ticker_to_name.values()]\n",
    "name_to_ticker = {name.lower(): symbol for symbol, name in ticker_to_name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#to be ran per comment\n",
    "def extract_ner_entities(model, text, similarity_threshold=90):\n",
    "    \n",
    "    BLACKLIST = {'ev', 'covid', 'etf', 'nyse', 'sec', 'spac', 'fda', 'treasury', 'covid-19', 'rrsp', 'tfsa','fed'}\n",
    "    doc = model(text)\n",
    "    detected_companies = []\n",
    "\n",
    "    #Detect companies via spaCy NER\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" and ent.text.lower() not in BLACKLIST:\n",
    "            org_name = ent.text.strip()\n",
    "            # Fuzzy match against official company names from csv file\n",
    "            match, score, _ = process.extractOne(org_name.lower(), company_names, scorer=fuzz.token_sort_ratio)\n",
    "            if score >= similarity_threshold:\n",
    "                matched_ticker = name_to_ticker[match]\n",
    "                canonical_name = ticker_to_name[matched_ticker]\n",
    "                detected_companies.append(canonical_name)\n",
    "            else:\n",
    "                #keeps companies not in csv file maybe delete later\n",
    "                detected_companies.append(org_name)\n",
    "\n",
    "    # --- Match stock tickers in text ---\n",
    "    for token in doc:\n",
    "        token_text = token.text.strip()\n",
    "\n",
    "        # Handle tickers with $ prefix, e.g. $AAPL\n",
    "        if token_text.startswith(\"$\"):\n",
    "            token_text = token_text[1:]\n",
    "\n",
    "        # Check if it’s a valid ticker symbol\n",
    "        if token_text in valid_tickers:\n",
    "            company_name = ticker_to_name.get(token_text)\n",
    "            detected_companies.append(company_name)\n",
    "\n",
    "\n",
    "    return list(set(detected_companies))\n",
    "\n",
    "\n",
    "def get_dict_top_companies(dataset, column_name, top_companies=10):\n",
    "    company_counter = dict()\n",
    "    for companies in dataset[column_name]:\n",
    "        for company in companies:\n",
    "            if company in company_counter:  \n",
    "                company_counter[company] += 1\n",
    "            else:\n",
    "                company_counter[company] = 1\n",
    "    sorted_dict = dict(sorted(company_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "    top = dict()\n",
    "    for company, count in list(sorted_dict.items())[:top_companies]:\n",
    "        top[company] = count\n",
    "\n",
    "    return top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure every cell is a string (NaN -> \"\")\n",
    "pre_processed_df['processed_text'] = pre_processed_df['processed_text'].fillna(\"\").astype(str)\n",
    "\n",
    "# Safe wrapper so extract_ner_entities always receives a string\n",
    "def safe_extract(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    return extract_ner_entities(nlp, text, similarity_threshold=60)\n",
    "\n",
    "pre_processed_df[\"Companies\"] = pre_processed_df['processed_text'].apply(safe_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Submissions ---\n",
    "top_dict = get_dict_top_companies(pre_processed_df, \"Companies\")\n",
    "top_set = set(top_dict.keys())\n",
    "print(f\"Top companies: {sorted(top_set)}\")\n",
    "\n",
    "masked = pre_processed_df['Companies'].apply(lambda lst: bool(top_set.intersection(lst)))\n",
    "filtered_df = pre_processed_df[masked].copy()\n",
    "\n",
    "exploded_sub = filtered_df.explode('Companies')\n",
    "\n",
    "# Keep only rows for top companies \n",
    "exploded_sub = exploded_sub[exploded_sub['Companies'].isin(top_set)].copy()\n",
    "\n",
    "# Create separate dataframes for each top company \n",
    "dfs_by_company = {}\n",
    "os.makedirs(\"companies_csv\", exist_ok=True)\n",
    "\n",
    "for company in top_set:\n",
    "    dfs_by_company[company] = exploded_sub[exploded_sub['Companies'] == company].copy()\n",
    "    file_path = os.path.join(\"companies_csv\", f\"{company}_submissions.csv\")\n",
    "    dfs_by_company[company].to_csv(file_path, index=False)\n",
    "    print(f\"{company}: {len(dfs_by_company[company])} rows saved to {file_path}\")\n",
    "\n",
    "print(\"\\nPreview of each company's SUBMISSIONS dataframe:\\n\")\n",
    "for company, df_company in dfs_by_company.items():\n",
    "    print(f\"=== {company.upper()} ({len(df_company)} rows) ===\")\n",
    "    display(df_company.head(3))   \n",
    "    print(\"\\n\")\n",
    "\n",
    "filtered_df.to_csv(\"filtered_top_companies.csv\", index=False)  #not really needed, just the dataframe with all top companies submissions\n",
    "print(\"Saved filtered_top_companies.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
