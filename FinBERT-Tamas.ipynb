{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19acf852",
   "metadata": {},
   "source": [
    "## FinBERT Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59ba6a",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23f394d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"    # avoid TF backend import\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"  # avoid Flax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81064472",
   "metadata": {},
   "source": [
    "#### FinBERT Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1986b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already loaded; skipping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL = \"ProsusAI/finbert\"\n",
    "\n",
    "if \"model\" not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded FinBERT on {DEVICE}.\")\n",
    "else:\n",
    "    # (optional) if CUDA becomes available later, move it\n",
    "    new_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if str(next(model.parameters()).device) != new_device:\n",
    "        model.to(new_device).eval()\n",
    "        DEVICE = new_device\n",
    "        print(f\"Moved model to {DEVICE}.\")\n",
    "    else:\n",
    "        print(\"Model already loaded; skipping.\")\n",
    "\n",
    "id2label = model.config.id2label  # {0:'positive',1:'negative',2:'neutral'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a863f",
   "metadata": {},
   "source": [
    "#### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bab579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert_proba(texts, batch_size=32, device=DEVICE, max_length=256):\n",
    "    \"\"\"\n",
    "    Return a list of dicts: [{'negative': p, 'neutral': p, 'positive': p}, ...]\n",
    "    \"\"\"\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            ).to(device)\n",
    "            logits = model(**enc).logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().tolist()\n",
    "            for p in probs:\n",
    "                # map indices -> labels dict\n",
    "                out = {id2label[j]: float(p[j]) for j in range(len(p))}\n",
    "                # ensure all three keys exist\n",
    "                for k in (\"negative\", \"neutral\", \"positive\"):\n",
    "                    out.setdefault(k, 0.0)\n",
    "                all_probs.append(out)\n",
    "    return all_probs\n",
    "\n",
    "# def sentiment_label(texts, neutrality_threshold=1):\n",
    "#     \"\"\"\n",
    "#     Map each text to 'positive'/'negative'/'neutral' after thresholding.\n",
    "#     \"\"\"\n",
    "#     probs = finbert_proba(texts)\n",
    "#     labels = []\n",
    "#     for p in probs:\n",
    "#         if p[\"neutral\"] >= neutrality_threshold:\n",
    "#             labels.append(\"neutral\")\n",
    "#         else:\n",
    "#             labels.append(\"positive\" if p[\"positive\"] > p[\"negative\"] else \"negative\")\n",
    "#     return labels\n",
    "\n",
    "def top2_margin(p: dict) -> float:\n",
    "    \"\"\"\n",
    "    Return max_prob - second_prob in [0,1].\n",
    "    \"\"\"\n",
    "    vals = sorted([p[\"negative\"], p[\"neutral\"], p[\"positive\"]], reverse=True)\n",
    "    return float(vals[0] - vals[1])\n",
    "\n",
    "def sentiment_score(texts):\n",
    "    \"\"\"\n",
    "    Continuous score per text: 0 if too neutral (currently 100% neutral or pos = neg), else (pos - neg).\n",
    "    Useful for weighting by Reddit score later.\n",
    "    \"\"\"\n",
    "    probs = finbert_proba(texts)\n",
    "    scores = []\n",
    "    for p in probs:\n",
    "        base = (p[\"positive\"] - p[\"negative\"])           \n",
    "        neutral_damp = (1.0 - p[\"neutral\"])              \n",
    "        margin_damp = top2_margin(p)                           \n",
    "        scores.append(base * neutral_damp * margin_damp)\n",
    "    return scores\n",
    "\n",
    "# tanh with 30D window\n",
    "def tanh_scale_series(scores, timestamps=None, percentile=95, min_periods=14):\n",
    "    \"\"\"\n",
    "    30D rolling P95 tanh scaling.\n",
    "    - scores: array-like of signed Reddit scores (ups - downs)\n",
    "    - timestamps: array-like of datetimes aligned with scores (if None -> global P95)\n",
    "    Returns:\n",
    "      w: np.ndarray in [-1, 1] (tanh-scaled, sign preserved)\n",
    "      k_used: np.ndarray of per-row k values (rolling P95 of |score|)\n",
    "    \"\"\"\n",
    "    s = np.asarray(scores, dtype=float)\n",
    "    if s.size == 0:\n",
    "        return s.astype(float), np.array([], dtype=float)\n",
    "    s = np.nan_to_num(s, nan=0.0)\n",
    "\n",
    "    if timestamps is not None:\n",
    "        ts = pd.to_datetime(pd.Series(timestamps), utc=True, errors=\"coerce\")\n",
    "        abs_s = pd.Series(np.abs(s), index=ts).sort_index()\n",
    "        roll_p = abs_s.rolling(\"30D\", min_periods=min_periods).quantile(percentile/100.0)\n",
    "        g_p = abs_s.quantile(percentile/100.0) if len(abs_s) else 1.0\n",
    "        k_series = roll_p.fillna(g_p).clip(lower=1.0)\n",
    "        # align per-row k back to original order (by timestamps)\n",
    "        k_used = k_series.reindex(ts).to_numpy()\n",
    "        # fallback for NaT rows\n",
    "        k_used = np.where(np.isfinite(k_used), k_used, max(g_p, 1.0))\n",
    "    else:\n",
    "        # global P95\n",
    "        g_p = np.percentile(np.abs(s), percentile) if s.size else 1.0\n",
    "        k_used = np.full_like(s, fill_value=max(g_p, 1.0), dtype=float)\n",
    "\n",
    "    w = np.tanh(s / k_used)\n",
    "    return w.astype(float), k_used.astype(float)\n",
    "\n",
    "# tanh scaling over whole dataset\n",
    "# def tanh_scale_series(scores, k=None, percentile=95):\n",
    "#     s = np.asarray(scores, dtype=float)\n",
    "#     s = np.nan_to_num(s, nan=0.0)\n",
    "#     if k is None:\n",
    "#         # choose a robust scale from your own data\n",
    "#         P = np.percentile(np.abs(s), percentile) if s.size else 1.0\n",
    "#         k = max(P, 1.0)\n",
    "#     w = np.tanh(s / k)  # w in [-1, 1], sign preserved\n",
    "#     return w, float(k)\n",
    "\n",
    "# without Tanh scaling.\n",
    "# def score_multiplier(raw_sentiment_scores, reddit_scores):\n",
    "#     \"\"\"\n",
    "#     Multiplies score per text by their Reddit score (Upvotes-Downvotes). \n",
    "#     \"\"\"\n",
    "#     scaled_reddit_scores = reddit_scores * w\n",
    "#     for i in range(len(raw_sentiment_scores)):\n",
    "#         scaled_sentiment_scores.append(raw_sentiment_scores[i] * scaled_reddit_scores[i])\n",
    "#     return scaled_sentiment_scores\n",
    "\n",
    "def score_multiplier(raw_sentiment_scores, reddit_scores, percentile=95):\n",
    "    \"\"\"\n",
    "    raw_sentiment_scores: list/array of sentiment scores in [-1, 1]\n",
    "    reddit_scores: list/array of signed Reddit scores (ups - downs)\n",
    "    returns: list of crowd-adjusted scores in [-1, 1]\n",
    "    \"\"\"\n",
    "    rs = np.asarray(raw_sentiment_scores, dtype=float)\n",
    "    w, k_used = tanh_scale_series(reddit_scores, percentile=percentile)\n",
    "    # elementwise combine; DO NOT multiply by raw reddit_scores again\n",
    "    return (rs * w).tolist(), k_used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835aa57",
   "metadata": {},
   "source": [
    "#### Running the Model on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc230e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proper, multiple datasets lines below\n",
    "file_locations ={\"Amazon\" : r\"C:\\Users\\ftomi\\Documents\\GitHub\\NLP-Group-10\\companies_csv\\Amazon_company_data.csv\", \n",
    "                 \"Apple\" : r\"C:\\Users\\ftomi\\Documents\\GitHub\\NLP-Group-10\\companies_csv\\Apple_company_data.csv\", \n",
    "                 \"Tesla\" : r\"C:\\Users\\ftomi\\Documents\\GitHub\\NLP-Group-10\\companies_csv\\Tesla_company_data.csv\"} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a729ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# singular test line below\n",
    "# file_locations = {\"Amazon\" : r\"C:\\Users\\ftomi\\Desktop\\NLP project\\Amazon_submissions.csv\"}\n",
    "\n",
    "for fl in file_locations:\n",
    "\n",
    "    df = pd.read_csv(file_locations[fl])\n",
    "\n",
    "    dates = []\n",
    "    texts = []\n",
    "    upvotes = [] \n",
    "    scaled_sentiment_scores = []\n",
    "    raw_sentiment_scores = []\n",
    "\n",
    "    data = df.to_dict('index')\n",
    "    for i in range(len(data)):\n",
    "        texts.append(data[i]['processed_text'])\n",
    "        dates.append(datetime.fromtimestamp(data[i]['created_utc'], tz=timezone.utc))\n",
    "        upvotes.append(data[i]['score'])\n",
    "\n",
    "    probs = finbert_proba(texts)\n",
    "    raw_sentiment_scores = sentiment_score(texts)\n",
    "    scaled_sentiment_scores, k_used = score_multiplier(sentiment_score(texts), upvotes)\n",
    "    \n",
    "    # save as dataframe\n",
    "    sentiment_df = pd.DataFrame(dict(scaled_sentiment = scaled_sentiment_scores, created_utc = dates))\n",
    "    sentiment_df.to_csv(f\"{fl}_sentiment_data.csv\", index=False)\n",
    "\n",
    "    #show and save figure\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(dates, raw_sentiment_scores, label = \"raw\",  linestyle=\":\", linewidth=0.8)\n",
    "    plt.plot(dates, scaled_sentiment_scores, label = \"scaled by reddit score\")\n",
    "    plt.title(f\"Sentiment Towards {fl} on r/stocks\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Total Sentiment\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{fl}_plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa433cd4",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d137c3",
   "metadata": {},
   "source": [
    "#### Sample Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12cf6484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon -> kept 14 rows from Amazon_company_data.csv\n",
      "Apple -> kept 20 rows from Apple_company_data.csv\n",
      "Tesla -> kept 20 rows from Tesla_company_data.csv\n",
      "\n",
      "Per-file rows kept:\n",
      "__source\n",
      "Apple     20\n",
      "Tesla     20\n",
      "Amazon    14\n",
      "Name: count, dtype: int64\n",
      "Wrote: FinBERT_TEST_SET.csv | rows: 54\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42) \n",
    "\n",
    "p = 0.002           \n",
    "\n",
    "samples = []\n",
    "\n",
    "for name, path in file_locations.items():   # dict: key=name, value=path\n",
    "    df = pd.read_csv(path, skiprows=lambda i: (i > 0) and (random.random() > p))\n",
    "    df[\"__source\"] = name  # so we can see contributions\n",
    "    print(f\"{name} -> kept {len(df):,} rows from {os.path.basename(path)}\")\n",
    "    samples.append(df)\n",
    "\n",
    "combined = pd.concat(samples, ignore_index=True)\n",
    "print(\"\\nPer-file rows kept:\")\n",
    "print(combined[\"__source\"].value_counts())\n",
    "\n",
    "combined.to_csv(\"FinBERT_TEST_SET.csv\", index=False)\n",
    "print(\"Wrote: FinBERT_TEST_SET.csv | rows:\", len(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aff209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4126fbc1",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2980f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = pd.read_csv(r\"C:\\Users\\ftomi\\Desktop\\NLP project\\comments_test_sample.csv\")\n",
    "test_submissions = pd.read_csv(r\"C:\\Users\\ftomi\\Desktop\\NLP project\\submissions_test_sample.csv\")\n",
    "\n",
    "def finbert_evaluation(df):\n",
    "    text_og = df[\"selftext\"].astype(str).tolist()\n",
    "    text_pp = df[\"processed_text\"].astype(str).tolist()\n",
    "    probs_og = finbert_proba(text_og, device=DEVICE)\n",
    "    probs_pp = finbert_proba(text_pp, device=DEVICE)\n",
    "    argmax_og = [max(p.items(), key=lambda kv: kv[1]) for p in probs_og]\n",
    "    argmax_pp = [max(p.items(), key=lambda kv: kv[1]) for p in probs_pp]\n",
    "    df[\"selftext_finbert_label\"] = [k for k, v in argmax_og]\n",
    "    df[\"selftext_finbert_score\"] = [v for k, v in argmax_og]\n",
    "    df[\"processed_finbert_label\"] = [k for k, v in argmax_pp]\n",
    "    df[\"processed_finbert_score\"] = [v for k, v in argmax_pp]\n",
    "    return df\n",
    "\n",
    "# outputs in SAME folder as inputs\n",
    "comments_test_complete = finbert_evaluation(test_comments)\n",
    "comments_test_complete.to_csv(\"comments_test_complete.csv\", index=False)\n",
    "subs_test_complete = finbert_evaluation(test_submissions)\n",
    "subs_test_complete.to_csv(\"submissions_test_complete.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf2ec0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMMENTS ===\n",
      "\n",
      "selftext_finbert_label  —  accuracy: 0.667\n",
      "selftext_finbert_label  negative  neutral  positive\n",
      "human_label                                        \n",
      "negative                       0        2         0\n",
      "neutral                        3        9         0\n",
      "positive                       0        0         1\n",
      "\n",
      "processed_finbert_label  —  accuracy: 0.800\n",
      "processed_finbert_label  negative  neutral  positive\n",
      "human_label                                         \n",
      "negative                        0        2         0\n",
      "neutral                         1       11         0\n",
      "positive                        0        0         1\n",
      "\n",
      "=== SUBMISSIONS ===\n",
      "\n",
      "selftext_finbert_label  —  accuracy: 0.600\n",
      "selftext_finbert_label  negative  neutral  positive\n",
      "human_label                                        \n",
      "negative                       0        2         0\n",
      "neutral                        1        9         0\n",
      "positive                       0        3         0\n",
      "\n",
      "processed_finbert_label  —  accuracy: 0.667\n",
      "processed_finbert_label  negative  neutral  positive\n",
      "human_label                                         \n",
      "negative                        0        2         0\n",
      "neutral                         0       10         0\n",
      "positive                        0        3         0\n"
     ]
    }
   ],
   "source": [
    "comments = pd.read_csv(r\"C:\\Users\\ftomi\\Desktop\\NLP project\\comments_test_complete.csv\")\n",
    "subs = pd.read_csv(r\"C:\\Users\\ftomi\\Desktop\\NLP project\\submissions_test_complete.csv\")\n",
    "\n",
    "# --- helpers ---\n",
    "def find_gold_col(df):\n",
    "    for name in [\"golden_standard_score\", \"Golden standard score\", \"golden_standard\", \"Golden standard\"]:\n",
    "        if name in df.columns: \n",
    "            return name\n",
    "    raise KeyError(\"Could not find the golden-standard column.\")\n",
    "\n",
    "def add_human_label(df):\n",
    "    df = df.copy()\n",
    "    gcol = find_gold_col(df)\n",
    "    scores = pd.to_numeric(df[gcol], errors=\"coerce\")\n",
    "    mapping = {1: \"positive\", 0: \"neutral\", -1: \"negative\"}\n",
    "    df[\"human_label\"] = scores.map(mapping)\n",
    "    return df\n",
    "\n",
    "def eval_preds(df, pred_cols):\n",
    "    \"\"\"Return accuracy + confusion matrices for each predicted label column.\"\"\"\n",
    "    out = {}\n",
    "    y_true = df[\"human_label\"].str.lower()\n",
    "    mask = y_true.notna()\n",
    "    for col in pred_cols:\n",
    "        y_pred = df[col].astype(str).str.lower()\n",
    "        acc = float((y_pred[mask] == y_true[mask]).mean())\n",
    "        cm = pd.crosstab(y_true[mask], y_pred[mask], dropna=False).reindex(\n",
    "            index=[\"negative\",\"neutral\",\"positive\"], columns=[\"negative\",\"neutral\",\"positive\"], fill_value=0\n",
    "        )\n",
    "        out[col] = {\"accuracy\": acc, \"confusion\": cm}\n",
    "    return out\n",
    "\n",
    "def print_report(name, results):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    for col, r in results.items():\n",
    "        print(f\"\\n{col}  —  accuracy: {r['accuracy']:.3f}\")\n",
    "        print(r[\"confusion\"])\n",
    "# ----------------\n",
    "\n",
    "# add human labels\n",
    "comments = add_human_label(comments)\n",
    "subs     = add_human_label(subs)\n",
    "\n",
    "# evaluate vs FinBERT labels (adjust column names if yours differ)\n",
    "comment_results = eval_preds(\n",
    "    comments, pred_cols=[\"selftext_finbert_label\", \"processed_finbert_label\"]\n",
    ")\n",
    "subs_results = eval_preds(\n",
    "    subs, pred_cols=[\"selftext_finbert_label\", \"processed_finbert_label\"]\n",
    ")\n",
    "\n",
    "print_report(\"COMMENTS\", comment_results)\n",
    "print_report(\"SUBMISSIONS\", subs_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
