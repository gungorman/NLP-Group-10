{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a30a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "'''def clean_company_name(name: str) -> str:\n",
    "    \"\"\"Normalize stock names to plain company names.\"\"\"\n",
    "    name = re.sub(r'\\bCommon Stock\\b', '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'\\bOrdinary Shares?\\b', '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'\\bDepositary Shares?.*', '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'\\bClass [A-Z]\\b', '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'\\bLtd\\b', 'Limited', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name'''\n",
    "\n",
    "# Load both ticker lists\n",
    "#nyse_df = pd.read_csv(\"nyse_screener.csv\")\n",
    "#nasdaq_df = pd.read_csv(\"nasdaq_screener.csv\")\n",
    "Top_100 = pd.read_csv(r\"C:\\Users\\lucas\\OneDrive\\Desktop\\NLP\\top_100_companies_2022.csv\", encoding='latin1')\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "#all_tickers = pd.concat([nyse_df, nasdaq_df], ignore_index=True)\n",
    "\n",
    "# Normalize column names just in case\n",
    "#all_tickers.columns = [col.strip().lower() for col in all_tickers.columns]\n",
    "Top_100.columns = [col.strip().lower() for col in Top_100.columns]\n",
    "\n",
    "# Clean names\n",
    "#all_tickers['clean_name'] = all_tickers['name'].apply(clean_company_name)\n",
    "\n",
    "# Create dictionaries for fast lookups\n",
    "ticker_to_name = dict(zip(Top_100['symbol'].str.upper(), Top_100['name']))\n",
    "valid_tickers = set(ticker_to_name.keys())\n",
    "company_names = [name.lower() for name in ticker_to_name.values()]\n",
    "name_to_ticker = {name.lower(): symbol for symbol, name in ticker_to_name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf6b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import warnings\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "#model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#to be ran per comment\n",
    "def extract_ner_entities(model, text, similarity_threshold=90):\n",
    "    \n",
    "    BLACKLIST = {'ev', 'covid', 'etf', 'nyse', 'sec', 'spac', 'fda', 'treasury', 'covid-19', 'rrsp', 'tfsa','fed', 'Reuters aggregated](https://www.streetinsider.com/Reuters'.lower()}\n",
    "    doc = model(text)\n",
    "    detected_companies = []\n",
    "\n",
    "    #Detect companies via spaCy NER\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" and ent.text.lower() not in BLACKLIST:\n",
    "            org_name = ent.text.strip()\n",
    "            # Fuzzy match against official company names from csv file\n",
    "            match, score, _ = process.extractOne(org_name.lower(), company_names, scorer=fuzz.token_sort_ratio)\n",
    "            if score >= similarity_threshold:\n",
    "                matched_ticker = name_to_ticker[match]\n",
    "                canonical_name = ticker_to_name[matched_ticker]\n",
    "                detected_companies.append(canonical_name)\n",
    "            else:\n",
    "                #keeps companies not in csv file maybe delete later\n",
    "                detected_companies.append(org_name)\n",
    "\n",
    "    # --- Match stock tickers in text ---\n",
    "    for token in doc:\n",
    "        token_text = token.text.strip()\n",
    "\n",
    "        # Handle tickers with $ prefix, e.g. $AAPL\n",
    "        if token_text.startswith(\"$\"):\n",
    "            token_text = token_text[1:]\n",
    "\n",
    "        # Check if itâ€™s a valid ticker symbol\n",
    "        if token_text in valid_tickers:\n",
    "            company_name = ticker_to_name.get(token_text)\n",
    "            detected_companies.append(company_name)\n",
    "\n",
    "\n",
    "    return list(set(detected_companies))\n",
    "\n",
    "\n",
    "def get_dict_top_companies(dataset, column_name, top_companies=3):\n",
    "    company_counter = dict()\n",
    "    for companies in dataset[column_name]:\n",
    "        for company in companies:\n",
    "            if company in company_counter:  \n",
    "                company_counter[company] += 1\n",
    "            else:\n",
    "                company_counter[company] = 1\n",
    "    sorted_dict = dict(sorted(company_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "    top = dict()\n",
    "    for company, count in list(sorted_dict.items())[:top_companies]:\n",
    "        top[company] = count\n",
    "\n",
    "    return top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc4fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': 320, 'Tesla': 281, 'Amazon': 274}\n"
     ]
    }
   ],
   "source": [
    "pre_processed_sub = pd.read_csv(r\"C:\\Users\\lucas\\OneDrive\\Desktop\\NLP\\processed_sub_2.csv\")\n",
    "pre_processed_com = pd.read_csv(r\"C:\\Users\\lucas\\OneDrive\\Desktop\\NLP\\processed_com.csv\")\n",
    "\n",
    "# Ensure every cell is a string (NaN -> \"\")\n",
    "pre_processed_sub[\"selftext\"] = pre_processed_sub[\"selftext\"].fillna(\"\").astype(str)\n",
    "pre_processed_com[\"selftext\"] = pre_processed_com[\"selftext\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Safe wrapper so extract_ner_entities always receives a string\n",
    "def safe_extract(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    return extract_ner_entities(nlp, text, similarity_threshold=60)\n",
    "\n",
    "pre_processed_sub[\"Companies\"] = pre_processed_sub[\"selftext\"].apply(safe_extract)\n",
    "pre_processed_com[\"Companies\"] = pre_processed_com[\"selftext\"].apply(safe_extract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Submissions ---\n",
    "import os\n",
    "\n",
    "\n",
    "top_dict_sub = get_dict_top_companies(pre_processed_sub, \"Companies\")\n",
    "top_set_sub = set(top_dict_sub.keys())\n",
    "print(f\"Top companies: {sorted(top_set_sub)}\")\n",
    "\n",
    "mask_sub = pre_processed_sub['Companies'].apply(lambda lst: bool(top_set_sub.intersection(lst)))\n",
    "filtered_df = pre_processed_sub[mask_sub].copy()\n",
    "\n",
    "exploded_sub = filtered_df.explode('Companies')\n",
    "\n",
    "# Keep only rows for top companies \n",
    "exploded_sub = exploded_sub[exploded_sub['Companies'].isin(top_set_sub)].copy()\n",
    "\n",
    "# Create separate dataframes for each top company \n",
    "dfs_by_company_sub = {}\n",
    "os.makedirs(\"companies_csv\", exist_ok=True)\n",
    "\n",
    "for company in top_set_sub:\n",
    "    dfs_by_company_sub[company] = exploded_sub[exploded_sub['Companies'] == company].copy()\n",
    "    file_path = os.path.join(\"companies_csv\", f\"{company}_submissions.csv\")\n",
    "    dfs_by_company_sub[company].to_csv(file_path, index=False)\n",
    "    print(f\"{company}: {len(dfs_by_company_sub[company])} rows saved to {file_path}\")\n",
    "\n",
    "print(\"\\nPreview of each company's SUBMISSIONS dataframe:\\n\")\n",
    "for company, df_company in dfs_by_company_sub.items():\n",
    "    print(f\"=== {company.upper()} ({len(df_company)} rows) ===\")\n",
    "    display(df_company.head(3))   \n",
    "    print(\"\\n\")\n",
    "\n",
    "filtered_df.to_csv(\"filtered_submissions_top_companies.csv\", index=False)  #not really needed, just the dataframe with all top companies submissions\n",
    "print(\"Saved filtered_submissions_top_companies.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d117fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comments ---\n",
    "import os\n",
    "\n",
    "\n",
    "top_dict_com = get_dict_top_companies(pre_processed_com, \"Companies\")\n",
    "top_set_com = set(top_dict_com.keys())\n",
    "print(f\"Top companies (comments): {sorted(top_set_com)}\")\n",
    "\n",
    "mask_com = pre_processed_com['Companies'].apply(lambda lst: bool(top_set_com.intersection(lst)))\n",
    "filtered_comments = pre_processed_com[mask_com].copy()\n",
    "\n",
    "print(f\"Comment rows before: {len(pre_processed_com)} | after filtering: {len(filtered_comments)}\")\n",
    "\n",
    "exploded_com = filtered_comments.explode('Companies')\n",
    "\n",
    "exploded_com = exploded_com[exploded_com['Companies'].isin(top_set_com)].copy()\n",
    "\n",
    "dfs_by_company_com = {}\n",
    "os.makedirs(\"companies_csv_comments\", exist_ok=True)\n",
    "\n",
    "for company in top_set_com:\n",
    "    dfs_by_company_com[company] = exploded_com[exploded_com['Companies'] == company].copy()\n",
    "    file_path = os.path.join(\"companies_csv_comments\", f\"{company}_comments.csv\")\n",
    "    dfs_by_company_com[company].to_csv(file_path, index=False)\n",
    "    print(f\"{company}: {len(dfs_by_company_com[company])} comment rows saved to {file_path}\")\n",
    "\n",
    "print(\"\\nPreview of each company's COMMENTS dataframe:\\n\")\n",
    "for company, df_company in dfs_by_company_com.items():\n",
    "    print(f\"=== {company.upper()} ({len(df_company)} rows) ===\")\n",
    "    display(df_company.head(3))\n",
    "    print()\n",
    "\n",
    "filtered_comments.to_csv(\"filtered_comments_top_companies.csv\", index=False)\n",
    "print(\"Saved filtered_comments_top_companies.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
